# DISCOVERIES.md

This file documents non-obvious problems, solutions, and patterns discovered during development. It serves as a living knowledge base.

**Archive**: Entries older than 3 months are moved to [DISCOVERIES_ARCHIVE.md](./DISCOVERIES_ARCHIVE.md).

## Table of Contents

### Recent (December 2025)

- [SessionStop Hook BrokenPipeError Race Condition](#sessionstop-hook-brokenpipeerror-race-condition-2025-12-13)
- [AI Agents Don't Need Human Psychology - No-Psych Winner](#ai-agents-dont-need-human-psychology-2025-12-02)
- [Mandatory User Testing Validates Its Own Value](#mandatory-user-testing-validates-value-2025-12-02)
- [System Metadata vs User Content in Git Conflict Detection](#system-metadata-vs-user-content-git-conflict-2025-12-01)

### November 2025

- [Power-Steering Session Type Detection Fix](#power-steering-session-type-detection-fix-2025-11-25)
- [Transcripts System Architecture Validation](#transcripts-system-investigation-2025-11-22)
- [Hook Double Execution - Claude Code Bug](#hook-double-execution-claude-code-bug-2025-11-21)
- [StatusLine Configuration Missing](#statusline-configuration-missing-2025-11-18)
- [Power-Steering Path Validation Bug](#power-steering-path-validation-bug-2025-11-17)
- [Power Steering Branch Divergence](#power-steering-mode-branch-divergence-2025-11-16)
- [Mandatory End-to-End Testing Pattern](#mandatory-end-to-end-testing-pattern-2025-11-10)
- [Neo4j Container Port Mismatch](#neo4j-container-port-mismatch-bug-2025-11-08)
- [Parallel Reflection Workstream Success](#parallel-reflection-workstream-execution-2025-11-05)

### October 2025

- [Pattern Applicability Framework](#pattern-applicability-analysis-framework-2025-10-20)
- [Socratic Questioning Pattern](#socratic-questioning-pattern-2025-10-18)
- [Expert Agent Creation Pattern](#expert-agent-creation-pattern-2025-10-18)

---

## SessionStop Hook BrokenPipeError Race Condition (2025-12-13)

### Problem

Amplihack hangs during Claude Code exit. User suspected sessionstop hook causing the hang. Investigation revealed the stop hook COMPLETES successfully but hangs when trying to write output back to Claude Code.

### Root Cause

**BrokenPipeError race condition in `hook_processor.py`**:

1. Stop hook completes all logic successfully (Neo4j cleanup, power-steering, reflection)
2. Returns `{"decision": "approve"}` and tries to write to stdout
3. Claude Code has already closed the pipe/connection (timing race)
4. `sys.stdout.flush()` at line 169 raises `BrokenPipeError: [Errno 32] Broken pipe`
5. Exception handler (line 308) catches it and tries to call `write_output({})` AGAIN at line 331
6. Second write also fails with BrokenPipeError (same broken pipe)
7. No handler for second failure â†’ HANG

**Evidence from logs:**

```
[2025-12-13T19:48:34] INFO: === STOP HOOK ENDED (decision: approve - no reflection) ===
[2025-12-13T19:48:34] ERROR: Unexpected error in stop: [Errno 32] Broken pipe
[2025-12-13T19:48:34] ERROR: Traceback:
  File "hook_processor.py", line 277: self.write_output(output)  â† FIRST FAILURE
  File "hook_processor.py", line 169: sys.stdout.flush()
BrokenPipeError: [Errno 32] Broken pipe
```

**Code Analysis:**

- `write_output()` called **4 times** (lines 277, 289, 306, 331) - all vulnerable
- **ZERO BrokenPipeError handling** anywhere in hooks directory
- Every exception handler tries to write output, potentially to broken pipe

### Solution

**Add BrokenPipeError handling to `write_output()` method in `hook_processor.py`:**

```python
def write_output(self, output: Dict[str, Any]):
    """Write JSON output to stdout, handling broken pipe gracefully."""
    try:
        json.dump(output, sys.stdout)
        sys.stdout.write("\n")
        sys.stdout.flush()
    except BrokenPipeError:
        # Pipe closed - Claude Code has already exited
        # Log but don't raise (fail-open design)
        self.log("Pipe closed during output write (non-critical)", "DEBUG")
    except IOError as e:
        if e.errno == errno.EPIPE:  # Broken pipe on some systems
            self.log("Broken pipe during output write (non-critical)", "DEBUG")
        else:
            raise  # Re-raise other IOErrors
```

**Alternative approach:** Wrap all `write_output()` calls in exception handlers (4 locations), but centralizing in the method is cleaner.

### Key Learnings

1. **Hook logic vs hook I/O** - The hook can work perfectly but fail on output writing
2. **Exception handlers can make things worse** - Retrying the same failed operation without checking
3. **Race conditions in pipe communication** - Claude Code can close pipes at ANY time, not just during hook logic
4. **Fail-open philosophy incomplete** - Recent fix (45778fcd) addressed `sys.exit(1)` but missed BrokenPipeError
5. **All hooks vulnerable** - Same `hook_processor.py` base class used by sessionstart, prompt-submit, etc.
6. **Log evidence is critical** - Hook completed successfully but logs showed BrokenPipeError during output write

### Related

- **File**: `.claude/tools/amplihack/hooks/hook_processor.py` (lines 161-169, 277, 289, 306, 331)
- **File**: `.claude/tools/amplihack/hooks/stop.py` (completes successfully, issue is in base class)
- **Recent fix**: Commit 45778fcd fixed `sys.exit(1)` but didn't address BrokenPipeError
- **Investigation methodology**: INVESTIGATION_WORKFLOW.md (6-phase systematic investigation)
- **Log evidence**: `.claude/runtime/logs/stop.log` shows "HOOK ENDED" followed by BrokenPipeError

### Remaining Questions

1. **Why does Claude Code close pipe prematurely?** (timeout? normal shutdown? hook execution time?)
2. **Frequency of race condition** - How often does this occur? Correlates with system load?
3. **Other hooks affected?** - All hooks use same base class, likely affects sessionstart too

---

## AI Agents Don't Need Human Psychology (2025-12-02)

### Problem

AI agents (Opus) achieving low workflow compliance (36-64% in early tests). Added psychological framing (Workflow Contract + Completion Celebration) to DEFAULT_WORKFLOW.md assuming it would help like it does for humans.

### Investigation

V8 testing: Builder agent created 5 worktrees with IDENTICAL content instead of 5 different variations. All had psychological elements REMOVED from DEFAULT_WORKFLOW.md (443 lines vs main's 482 lines).

### Discovery

**Removing psychological framing improves AI performance 72-95%**:

- MEDIUM: $2.93-$8.36 (avg $5.62), 100% compliance
- HIGH: $13.56-$31.95 (avg $21.72), 100% compliance
- Annual impact: ~$123K savings (90% reduction)

**Elements Removed** (39 lines):

1. Workflow Contract (lines 30-47): Commitment language
2. Completion Celebration (lines 462-482): Success celebration

### Root Cause

**Human psychology â‰  AI optimization**:

- AI already committed by design (psychology unnecessary)
- AI don't experience celebration (wasted tokens)
- Psychology = 8% overhead, 0% benefit for AI
- Less = more (token efficiency)

### Solution

**Remove psychological framing from AI-facing workflows**:

```markdown
# BEFORE (482 lines, WITH Psychology)

## Workflow Contract

By reading this workflow file, you are committing...
[17 lines of commitment psychology]

[22 Workflow Steps]

## ðŸŽ‰ Workflow Complete!

Congratulations! You executed all 22 steps...
[22 lines of celebration psychology]

# AFTER (443 lines, WITHOUT Psychology)

[22 Workflow Steps - just the steps, no psychology]
```

### Validation

- Tests: 7 (3 MEDIUM + 4 HIGH complexity)
- Quality: 100% compliance (22/22 steps every test)
- Variance: High (136-185%) but averages excellent
- Philosophy: "Code you don't write has no bugs" applies to prompts!

### Impact

**Immediate**: 72-95% cost reduction, 76-90% time reduction
**Annual**: ~$123K saved, ~707 hours (18 work weeks)
**Quality**: 100% maintained (no degradation)

### Lessons

1. Don't assume human psychology helps AI - test first
2. Less is more for AI agents - remove non-essential
3. Apply philosophy to prompts - ruthless simplicity works
4. Builder can apply philosophy - autonomously removed complexity, was correct!
5. Forensic analysis essential - 3 wrong attributions before file diff revealed truth

### Related

- Issue #1785 (V8 testing results)
- Tag: v8-no-psych-winner
- Archive: .claude/runtime/benchmarks/v8_experiments_archive_20251202_212646/
- Docs: /tmp/â­_START_HERE_ULTIMATE_GUIDE.md

---

## Entry Format Template

```markdown
## [Brief Title] (YYYY-MM-DD)

### Problem

What challenge was encountered?

### Root Cause

Why did this happen?

### Solution

How was it resolved? Include code if relevant.

### Key Learnings

What insights should be remembered?
```

---

## System Metadata vs User Content in Git Conflict Detection (2025-12-01)

### Problem

User reported: "amplihack's copytree_manifest fails when .claude/ has uncommitted changes" specifically with `.claude/.version` file modified. Despite having a comprehensive safety system (GitConflictDetector + SafeCopyStrategy), deployment proceeded without warning and created a version mismatch state.

### Root Cause

The `.version` file is a **system-generated tracking file** that stores the git commit hash of the deployed amplihack package. The issue occurred due to a semantic classification gap:

1. **Git Status Detection**: `GitConflictDetector._get_uncommitted_files()` correctly detects ALL uncommitted files including `.version` (status: M)

2. **Filtering Logic Gap**: `_filter_conflicts()` at lines 82-97 in `git_conflict_detector.py` only checks files against ESSENTIAL_DIRS patterns:

   ```python
   for essential_dir in essential_dirs:
       if relative_path.startswith(essential_dir + "/"):
           conflicts.append(file_path)
   ```

3. **ESSENTIAL_DIRS Are All Subdirectories**: `["agents/amplihack", "commands/amplihack", "context/", ...]` - all contain "/"

4. **Root-Level Files Filtered Out**: `.version` at `.claude/.version` doesn't match any pattern â†’ filtered OUT â†’ `has_conflicts = False`

5. **No Warning Issued**: SafeCopyStrategy sees no conflicts, proceeds to working directory without prompting user

6. **Version Mismatch Created**: copytree_manifest copies fresh directories but **doesn't copy `.version`** (not in ESSENTIAL_FILES), leaving stale version marker with fresh code

### Solution

Exclude system-generated metadata files from conflict detection by adding explicit categorization:

```python
# In src/amplihack/safety/git_conflict_detector.py

SYSTEM_METADATA = {
    ".version",        # Framework version tracking (auto-generated)
    "settings.json",   # Runtime settings (auto-generated)
}

def _filter_conflicts(
    self, uncommitted_files: List[str], essential_dirs: List[str]
) -> List[str]:
    """Filter uncommitted files for conflicts with essential_dirs."""
    conflicts = []
    for file_path in uncommitted_files:
        if file_path.startswith(".claude/"):
            relative_path = file_path[8:]

            # Skip system-generated metadata - safe to overwrite
            if relative_path in SYSTEM_METADATA:
                continue

            # Existing filtering logic for essential directories
            for essential_dir in essential_dirs:
                if (
                    relative_path.startswith(essential_dir + "/")
                    or relative_path == essential_dir
                ):
                    conflicts.append(file_path)
                    break
    return conflicts
```

**Rationale**:

- **Semantic Classification**: Filter by PURPOSE (system vs user), not just directory structure
- **Ruthlessly Simple**: 3-line change, surgical fix
- **Philosophy-Aligned**: Treats system files appropriately (not user content)
- **Zero-BS**: Fixes exact issue without over-engineering

### Key Learnings

1. **Root-Level Files Need Special Handling**: Directory-based filtering (checking for "/") misses root-level files entirely. System metadata often lives at root.

2. **Semantic > Structural Classification**: Git conflict detection should categorize by FILE PURPOSE (user-managed vs system-generated), not just location patterns.

3. **Auto-Generated Files vs User Content**: Framework metadata files like `.version`, `*.lock`, `.state` should never trigger conflict warnings - they're infrastructure, not content.

4. **ESSENTIAL_DIRS Pattern Limitation**: Works great for subdirectories (`context/`, `tools/`), but silently excludes root-level files. Need explicit system file list.

5. **False Negatives Are Worse Than False Positives**: Safety system failing to warn about user content is bad, but warning about system files breaks user trust and workflow.

6. **Version Files Are Special**: Any framework with version tracking faces this - `.version`, `.state`, `.lock` files should be treated as disposable metadata, not user content to protect.

### Related Patterns

- See PATTERNS.md: "System Metadata vs User Content Classification" - NEW pattern added from this discovery
- Relates to "Graceful Environment Adaptation" (different file handling per environment)
- Reinforces "Fail-Fast Prerequisite Checking" (but needs correct semantic classification)

### Impact

- **Affects**: All deployments where `.version` or other system metadata has uncommitted changes
- **Frequency**: Common after updates (`.version` auto-updated but not committed)
- **User Experience**: Confusing "version mismatch" errors despite fresh deployment
- **Fix Priority**: High - breaks user trust in safety system

### Verification

Test cases added:

- Uncommitted `.version` doesn't trigger conflict warning âœ…
- Uncommitted user content (`.claude/context/custom.md`) DOES trigger warning âœ…
- Deployment proceeds smoothly with modified `.version` âœ…
- Version mismatch detection still works correctly âœ…

---

## Auto Mode Timeout Causing Opus Model Workflow Failures (2025-11-26)

### Problem

Opus model was "skipping" workflow steps during auto mode execution. Investigation revealed the 5-minute per-turn timeout was cutting off Opus execution mid-workflow due to extended thinking requirements.

### Root Cause

The default per-turn timeout of 5 minutes was too aggressive for Opus model, which requires extended thinking time. Log analysis showed:

- `Turn 2 timed out after 300.0s`
- `Turn 1 timed out after 600.1s`

### Solution (PR #1676)

Implemented flexible timeout resolution system:

1. **Increased default timeout**: 5 min â†’ 30 min
2. **Added `--no-timeout` flag**: Disables timeout entirely using `nullcontext()`
3. **Opus auto-detection**: Model names containing "opus" automatically get 60 min timeout
4. **Clear priority system**: `--no-timeout` > explicit > auto-detect > default

### Key Insight

**Extended thinking models like Opus need significantly longer timeouts.** Auto-detection based on model name provides a good default without requiring users to remember to adjust settings.

### Files Changed

- `src/amplihack/cli.py`: Added `--no-timeout` flag and `resolve_timeout()` function
- `src/amplihack/launcher/auto_mode.py`: Accept `None` timeout using `nullcontext`
- `tests/unit/test_auto_mode_timeout.py`: 19 comprehensive tests
- `docs/AUTO_MODE.md`: Added timeout configuration documentation

## Power-Steering Session Type Detection Fix (2025-11-25)

### Problem

Power-steering incorrectly blocking investigation sessions with development-specific checks. Sessions like "Investigate SSH issues" were misclassified as DEVELOPMENT.

### Root Cause

`detect_session_type()` relied solely on tool-based heuristics. Troubleshooting sessions involve Bash commands and doc updates, matching development patterns.

### Solution

Added **keyword-based detection** with priority over tool heuristics. Check first 5 user messages for investigation keywords (investigate, troubleshoot, diagnose, debug, analyze).

### Key Learnings

**User intent (keywords) is more reliable than tool usage patterns** for session classification.

---

## Transcripts System Investigation (2025-11-22)

### Problem

Needed validation of amplihack's transcript architecture vs Microsoft Amplifier approach.

### Key Findings

- **Decision**: Maintain current 2-tier builder architecture
- **Rationale**: Perfect philosophy alignment (30/30) + proven stability
- **Architecture**: ClaudeTranscriptBuilder + CodexTranscriptsBuilder with 4 strategic hooks
- **5 advantages over Amplifier**: Session isolation, human-readable Markdown, fail-safe architecture, original request tracking, zero external dependencies

### Key Learnings

Independent innovation can be better than adopting external patterns. Session isolation beats centralized state.

---

## Hook Double Execution - Claude Code Bug (2025-11-21)

### Problem

SessionStart and Stop hooks execute **twice per session** with different PIDs.

### Root Cause

**Claude Code internal bug #10871** - Hook execution engine spawns two separate processes regardless of configuration. Our config is correct per schema.

### Solution

**NO CODE FIX AVAILABLE**. Accept duplication as known limitation. Hooks are idempotent, safe but wasteful (~2 seconds per session).

### Key Learnings

1. Configuration was correct - the `"hooks": []` wrapper is required by schema
2. Schema validation prevents incorrect "fixes"
3. Upstream bugs affect downstream projects

**Tracking**: Claude Code GitHub Issue #10871

---

## StatusLine Configuration Missing (2025-11-18)

### Problem

Custom status line feature fully implemented but never configured during installation.

### Root Cause

Both installation templates (install.sh and uvx_settings_template.json) excluded statusLine configuration.

### Solution (Issue #1433)

Added statusLine config to both templates with appropriate path formats.

### Key Learnings

Feature discoverability requires installation automation. Templates should match feature implementations.

---

## Power-Steering Path Validation Bug (2025-11-17)

### Problem

Power-steering fails with path validation error. Claude Code stores transcripts in `~/.claude/projects/` which is outside project root.

### Root Cause

`_validate_path()` too strict - only allows project root and temp directories.

### Solution

Whitelist `~/.claude/projects/` directory in path validation.

### Key Learnings

1. **Agent orchestration works for complex debugging**: Specialized agents
   (architect, reviewer, security) effectively decomposed the problem
2. **Silent failures need specialized detection**: Merge conflicts blocking
   tools require dedicated diagnostic capabilities
3. **Environment parity is critical**: Version mismatches cause significant
   investigation overhead (20-25 minutes)
4. **Pattern recognition accelerates resolution**: Known patterns should be
   automated
5. **Time-to-discovery varies by issue type**: Merge conflicts (10 min) vs
   version mismatches (25 min)
6. **Documentation discipline enables learning**: Having PHILOSOPHY.md,
   PATTERNS.md available accelerated analysis

### Prevention

**Immediate improvements needed**:

- **CI Diagnostics Agent**: Automated environment comparison and version
  mismatch detection
- **Silent Failure Detector Agent**: Pre-commit hook validation and merge
  conflict detection
- **Pattern Recognition Agent**: Automated matching to historical failure
  patterns

**Process improvements**:

- Environment comparison should be step 1 in CI failure debugging
- Check merge conflicts before running any diagnostic tools
- Use parallel agent execution for faster diagnosis
- Create pre-flight checks before CI submission

**New agent delegation triggers**:

- CI failures â†’ CI Diagnostics Agent
- Silent tool failures â†’ Silent Failure Detector Agent
- Recurring issues â†’ Pattern Recognition Agent

**Target performance**: Reduce 45-minute complex debugging to 20-25 minutes
through automation and specialized agents.

---

## Claude-Trace UVX Argument Passthrough Issue (2025-09-26)

### Issue

UVX argument passthrough was failing for claude-trace integration. Commands like
`uvx --from git+... amplihack -- -p "prompt"` would launch interactively instead
of executing the prompt directly, forcing users to manually enter prompts.

### Root Cause

**Misdiagnosis Initially**: Thought issue was with UVX argument parsing, but
`parse_args_with_passthrough()` was working correctly.

**Actual Root Cause**: Command building logic in
`ClaudeLauncher.build_claude_command()` wasn't handling claude-trace syntax
properly. Claude-trace requires different command structure:

- **Standard claude**: `claude --dangerously-skip-permissions -p "prompt"`
- **Claude-trace**:
  `claude-trace --run-with chat --dangerously-skip-permissions -p "prompt"`

The key difference is claude-trace needs `--run-with chat` before Claude
arguments.

### Solution

Modified `src/amplihack/launcher/core.py` in `build_claude_command()` method:

```python
if claude_binary == "claude-trace":
    # claude-trace requires --run-with followed by the command and arguments
    # Format: claude-trace --run-with chat [claude-args...]
    cmd = [claude_binary, "--run-with", "chat"]

    # Add Claude arguments after the command
    cmd.append("--dangerously-skip-permissions")

    # Add system prompt, --add-dir, and forwarded arguments...
    if self.claude_args:
        cmd.extend(self.claude_args)

    return cmd
```

### Key Learnings

1. **Tool-specific syntax matters**: Different tools (claude vs claude-trace)
   may require completely different argument structures even when functionally
   equivalent
2. **Debugging scope**: Initially focused on argument parsing when the issue was
   in command building - trace through the entire pipeline
3. **Integration complexity**: Claude-trace integration adds syntax complexity
   that must be handled explicitly
4. **Testing real scenarios**: Mock testing wasn't sufficient - needed actual
   UVX deployment testing to catch this
5. **Command structure precedence**: Some tools require specific argument
   ordering (--run-with must come before other args)

### Prevention

- **Always test real deployment scenarios**: Don't rely only on local testing
  when tools have different deployment contexts
- **Document tool-specific syntax requirements**: Create clear examples for each
  supported execution mode
- **Test command building separately**: Unit test command construction logic
  independently from argument parsing
- **Integration testing**: Include UVX deployment testing in CI/CD pipeline
- **Clear error messages**: Provide better feedback when argument passthrough
  fails

### Pattern Recognition

**Trigger Signs of Command Structure Issues:**

- Arguments parsed correctly but command fails silently
- Tool works interactively but not with arguments
- Different behavior between direct execution and wrapper tools
- Integration tools requiring specific argument ordering

**Debugging Approach:** When argument passthrough fails:

1. Verify argument parsing is working (log parsed args)
2. Check command building logic (log generated command)
3. Test command manually to isolate syntax issues
4. Compare tool documentation for syntax differences

### Testing Validation

All scenarios now working:

- âœ… `uvx amplihack -- --help` (shows Claude help)
- âœ… `uvx amplihack -- -p "Hello world"` (executes prompt)
- âœ… `uvx amplihack -- --model claude-3-opus-20240229 -p "test"` (model +
  prompt)

**Issue**: #149 **PR**: #150 **Branch**:
`feat/issue-149-uvx-argument-passthrough`

---

## Socratic Questioning Pattern for Knowledge Exploration (2025-10-18)

### Issue

Need effective method for generating deep, probing questions that challenge technical claims and surface hidden assumptions in knowledge-builder scenarios.

### Root Cause

Simple question generation often produces shallow inquiries that can be deflected or answered superficially. Effective Socratic questioning requires strategic multi-dimensional attack on claims combined with formal precision.

### Solution

**Three-Dimensional Question Attack Strategy**:

1. **Empirical Dimension**: Challenge with observable evidence and historical outcomes
   - Example: "Why do memory safety bugs persist despite 30 years of tool development?"
   - Grounds abstract claims in reality
   - Hard to dismiss as "merely theoretical"

2. **Computational Dimension**: Probe tractability and complexity
   - Example: "Does manual discipline require solving NP-complete problems in your head?"
   - Connects theory to practical cognitive limitations
   - Reveals fundamental constraints

3. **Formal Mathematical Dimension**: Demand precise relationships
   - Example: "Is the relationship bijective or a subset? What's lost?"
   - Forces rigorous thinking
   - Prevents vague equivalence claims

**Key Techniques**:

- Use formal language (bijective, NP-complete) to force precision
- Embed context within questions to prevent deflection
- Connect theoretical claims to observable outcomes
- Attack different aspects of claim simultaneously (cannot defend all fronts equally)

### Key Learnings

1. **Multi-dimensional attack is more effective than single-angle questioning** - Forces comprehensive defense
2. **Formal language prevents hand-waving** - "Bijective" demands precision that "similar" doesn't
3. **Empirical grounding matters** - Observable outcomes harder to dismiss than pure theory
4. **Question length/complexity tradeoff** - Longer questions with embedded context are acceptable for deep exploration
5. **Pattern is domain-agnostic** - Works for technical debates, philosophical claims, design decisions

### Usage Context

**When to Use**:

- Knowledge-builder agent scenarios requiring deep exploration
- Challenging technical or philosophical claims
- Surfacing hidden assumptions in design decisions
- Teaching critical thinking through guided inquiry

**When NOT to Use**:

- Simple factual questions (overkill)
- Time-sensitive decisions (too slow)
- Consensus-building conversations (too confrontational)

### Evidence Status

**Proven**: 1 successful usage (memory safety ownership vs. discipline debate)
**Needs**: 2-3 more successful applications before promoting to PATTERNS.md
**Next Test**: Use with knowledge-builder agent in actual knowledge exploration scenario

### Prevention

**To implement effectively**:

- Test pattern 2-3 more times in varied contexts
- Validate with knowledge-builder agent integration
- Refine based on actual usage feedback
- Consider adding to PATTERNS.md after sufficient validation

**Trigger Signs for Pattern Use**:

- User makes strong equivalence claim ("X is just Y")
- Need to explore assumptions systematically
- Goal is deep understanding, not quick answers
- Conversational context allows longer-form inquiry

---

## Pattern Applicability Analysis Framework (2025-10-20)

### Discovery

Through analysis of PBZFT vs N-Version Programming decision, identified six reusable meta-patterns for evaluating when to adopt patterns from other domains, particularly distributed systems patterns applied to AI agent systems.

### Context

Considered implementing PBZFT (Practical Byzantine Fault Tolerance) as new amplihack pattern after comparison with existing N-Version Programming approach. Analysis revealed PBZFT would be 6-9x more complex with zero benefit, leading to systematic exploration of WHY this mismatch occurred and how to prevent similar mistakes.

### Pattern 1: Threat Model Precision Principle

**Core Insight**: Fault tolerance mechanisms are only effective when matched to correct threat model. Mismatched defenses add complexity without benefit.

**Decision Framework**:

1. Identify actual failure mode (what really breaks?)
2. Classify threat type: Honest mistakes vs Malicious intent
3. Match defense mechanism to threat type
4. Reject mechanisms designed for different threats

**Evidence**:

- PBZFT defends against Byzantine failures (malicious nodes) - not our threat
- N-Version defends against independent errors (honest mistakes) - our actual threat
- Voting defends against adversaries; expert review catches quality issues

**Anti-Pattern**: Applying "industry standard" solutions without verifying threat match

### Pattern 2: Voting vs Expert Judgment Selection Criteria

**Core Insight**: Voting and expert judgment serve fundamentally different purposes and produce different quality outcomes.

**When Voting Works**:

- Adversarial environment (can't trust individual nodes)
- Binary or simple discrete choices
- No objective quality metric available
- Consensus more valuable than correctness

**When Expert Judgment Works**:

- Cooperative environment (honest actors)
- Complex quality dimensions (code quality, architecture)
- Objective evaluation criteria exist
- Correctness more valuable than consensus

**Evidence**:

- Code quality has measurable dimensions (complexity, maintainability, correctness)
- Expert review provides detailed feedback ("Fix this specific issue")
- Voting provides only rejection ("This is wrong") without guidance
- N-Version achieves 30-65% error reduction through diversity, not voting

**Application**: Code review, architectural decisions, security audits all benefit from expert judgment over democratic voting.

### Pattern 3: Distributed Systems Pattern Applicability Test

**Core Insight**: Many distributed systems patterns don't apply to AI agent systems due to different coordination models and failure characteristics.

**Critical Differences**:

| Dimension     | Distributed Systems | AI Agent Systems        |
| ------------- | ------------------- | ----------------------- |
| Node Behavior | Can be malicious    | Honest but imperfect    |
| Failure Mode  | Byzantine faults    | Independent errors      |
| Coordination  | Explicit consensus  | Implicit through design |
| Communication | Messages, network   | Shared specifications   |
| Trust Model   | Zero-trust          | Cooperative             |

**Applicability Test Questions**:

1. Does pattern assume adversarial nodes? (Usually doesn't apply to AI)
2. Does pattern solve network partition issues? (AI agents share state)
3. Does pattern require message passing? (AI agents use shared context)
4. Does pattern optimize for communication cost? (AI has different cost model)

**Patterns That DO Apply to AI**:

- Load balancing (parallel agent execution)
- Caching (memoization, state management)
- Event-driven architecture (hooks, triggers)
- Circuit breakers (fallback strategies)

**Patterns That DON'T Apply to AI**:

- Byzantine consensus (PBZFT, blockchain)
- CAP theorem considerations (no network partitions)
- Gossip protocols (agents don't need eventual consistency)
- Quorum systems (voting inferior to expert review)

### Pattern 4: Complexity-Benefit Trade-off Quantification

**Core Insight**: Complex solutions must provide proportional benefit. Simple metrics reveal when complexity is unjustified.

**Quantification Framework**:

```
Complexity Cost = (Lines of Code) Ã— (Conceptual Overhead) Ã— (Integration Points)
Benefit Gain = (Problem Solved) Ã— (Quality Improvement) Ã— (Risk Reduction)

Justified Complexity: Benefit Gain / Complexity Cost > 3.0
```

**Evidence**:

- PBZFT: 6-9x complexity increase, 0x benefit (solves non-existent problem)
- N-Version: 2x complexity increase, 30-65% error reduction

**Red Flags for Unjustified Complexity**:

- Complexity ratio > 3x with no measurable benefit
- Solution requires understanding concepts not needed elsewhere
- Implementation needs extensive documentation to explain
- "Industry best practice" argument without context validation

### Pattern 5: Domain Appropriateness Check for "Best Practices"

**Core Insight**: Best practices from one domain can be anti-patterns in another. Always validate domain appropriateness before adopting.

**Validation Checklist**:

- Does this practice solve a problem that exists in MY domain?
- Does my domain have same threat model as source domain?
- Are constraints that drove this practice present in my system?
- What was original context that made this "best"?
- Has this been proven effective in contexts similar to mine?

**Common Cross-Domain Misapplications**:

- Microservices patterns â†’ Monolithic apps (unnecessary distribution)
- Blockchain consensus â†’ Database systems (unnecessary Byzantine tolerance)
- Military security â†’ Consumer apps (disproportionate paranoia)
- Enterprise architecture â†’ Startups (premature abstraction)

**Protection Strategy**:

1. Ask: "What problem does this solve in THAT domain?"
2. Verify: "Do I have that same problem?"
3. Question: "What are costs of importing this pattern?"
4. Consider: "Is there simpler solution that fits MY constraints?"

### Pattern 6: Diversity as Error Reduction Mechanism

**Core Insight**: Independent diverse implementations naturally reduce correlated errors without requiring voting or consensus mechanisms.

**How Diversity Works**:

- N diverse implementations of same specification
- Each has probability p of independent error
- Probability of same error in all N: p^N (exponential reduction)
- No voting required - diversity itself provides value

**Evidence**:

- N-Version provides 30-65% error reduction through diversity alone
- PBZFT's voting adds complexity without increasing diversity benefit
- Expert review can select best implementation after diversity generation

**Application to AI Agents**:

```python
# Generate diverse implementations
implementations = [
    agent1.generate(spec),
    agent2.generate(spec),
    agent3.generate(spec),
]

# Expert review selects best (not voting)
best = expert_reviewer.select_best(implementations, criteria)
```

**When Diversity Fails**:

- Specifications are ambiguous (correlated errors from misunderstanding)
- Common dependencies (same libraries, same bugs)
- Shared misconceptions (all agents trained on similar data)

### Meta-Pattern: Systematic Pattern Applicability Analysis

**Five-Phase Framework** for evaluating pattern adoption:

**Phase 1: Threat Model Match**

- Identify actual failure modes in target system
- Identify pattern's target failure modes
- Verify failure modes match
- If mismatch, REJECT pattern

**Phase 2: Mechanism Appropriateness**

- Does pattern use voting? (Usually wrong for quality assessment)
- Does pattern assume adversarial behavior? (Usually wrong for AI)
- Does pattern optimize for network communication? (Usually irrelevant for AI)
- Does pattern solve target domain's specific problem?

**Phase 3: Complexity Justification**

- Calculate complexity increase (lines, concepts, integration points)
- Measure benefit gain (error reduction, risk mitigation)
- Verify benefit/complexity ratio > 3.0
- If ratio < 3.0, seek simpler alternatives

**Phase 4: Domain Validation**

- Research pattern's origin domain
- Understand original context and constraints
- Verify target domain shares those characteristics
- Check for successful applications in similar domains

**Phase 5: Alternative Exploration**

- Brainstorm domain-native solutions
- Can simpler mechanisms achieve same benefits?
- What would "ruthless simplicity" approach look like?
- Can you get 80% of benefit with 20% of complexity?

### Key Learnings

1. **Threat model mismatch is primary source of inappropriate pattern adoption** - Always verify failure modes match before importing patterns
2. **Voting and expert judgment are not interchangeable** - Code quality requires expert review, not democratic voting
3. **Distributed systems patterns rarely map to AI systems** - Different trust models, coordination mechanisms, and failure characteristics
4. **Complexity must be proportionally justified** - 3:1 benefit-to-cost ratio minimum for adopting complex patterns
5. **Best practices are domain-specific** - What's "best" in blockchain may be anti-pattern for AI agents
6. **Diversity reduces errors without consensus overhead** - N-Version's power comes from diversity, not voting

### Prevention

**Before adopting any pattern from another domain:**

1. Run through five-phase applicability analysis
2. Verify threat model match as first step
3. Calculate complexity-to-benefit ratio
4. Question "industry best practice" claims
5. Explore domain-native alternatives
6. Default to ruthless simplicity unless complexity clearly justified

**Red Flags Requiring Deep Analysis**:

- Pattern from adversarial domain (blockchain, security) â†’ cooperative domain (AI agents)
- Pattern optimizes for constraints not present in target (network latency, Byzantine nodes)
- Complexity increase > 3x without measurable benefit
- "Everyone uses this" without context-specific validation

### Integration with Existing Philosophy

This discovery strengthens and validates existing principles:

- **Ruthless Simplicity** (PHILOSOPHY.md): Complexity must justify existence
- **Zero-BS Implementation** (PHILOSOPHY.md): No solutions for non-existent problems
- **Question Everything** (PHILOSOPHY.md): Challenge "best practices" without validation
- **Necessity First** (PHILOSOPHY.md): "Do we actually need this right now?"

### Files Referenced

- PBZFT Analysis: `.claude/runtime/logs/[session]/pbzft_analysis.md`
- N-Version Pattern: Already implemented in amplihack
- Threat Modeling: Aligns with security agent principles
- Complexity Analysis: Informed by PHILOSOPHY.md simplicity mandate

### Next Steps

1. **Apply pattern applicability framework** when evaluating future pattern adoptions
2. **Create checklist tool** for systematic pattern evaluation
3. **Document known anti-patterns** from inappropriate domain transfers
4. **Strengthen agent instructions** to question pattern applicability before implementation

### Related Patterns

- Connects to "Ruthless Simplicity" (PHILOSOPHY.md)
- Enhances "Decision-Making Framework" (PHILOSOPHY.md)
- Validates "Question Everything" principle
- Extends pattern recognition capabilities

---

## Massive Parallel Reflection Workstream Execution (2025-11-05)

### Context

Successfully executed 13 parallel full-workflow tasks simultaneously, converting reflection system findings into GitHub issues and implementing solutions. This represented the largest parallel agent execution to date, demonstrating the scalability and robustness of the workflow system.

### Discovery

**Parallel Execution at Scale Works**: Successfully managed 13 concurrent feature implementations (issues #1089-#1101) using worktree isolation, each following the complete 13-step workflow from planning through PR creation.

**Key Metrics:**

- 13 issues created from reflection analysis
- 13 feature branches via git worktrees
- 13 PRs created with 9-10/10 philosophy compliance
- 7 message reduction features addressing real pain points
- 100% success rate (no failed workflows)

### Root Cause Analysis

**Why This Succeeded:**

1. **Worktree Isolation**: Each feature in separate worktree prevented cross-contamination
   - Branch: `feat/issue-{number}-{description}`
   - Location: `/tmp/worktree-issue-{number}`
   - No merge conflicts between parallel operations

2. **Agent Specialization**: Each workflow step delegated to appropriate agents
   - prompt-writer: Requirements clarification
   - architect: Design specifications
   - builder: Implementation
   - reviewer: Quality assurance
   - fix-agent: Conflict resolution

3. **Fix-Agent Pattern**: Systematic conflict resolution using templates
   - Cherry-pick strategy for divergent branches
   - Pattern-based resolution (import, config, quality)
   - Quick mode for common issues

4. **Documentation-First Approach**: Templates and workflows provided clear guidance
   - Message templates (.claude/data/message_templates/)
   - Fix templates (.claude/data/fix_templates/)
   - Workflow documentation (docs/workflows/)

### Solution Patterns That Worked

**1. Worktree Management Pattern:**

```bash
# Create isolated workspace
git worktree add /tmp/worktree-issue-{N} -b feat/issue-{N}-{description}

# Work independently
cd /tmp/worktree-issue-{N}
# ... implement feature ...

# Push and create PR
git push -u origin feat/issue-{N}-{description}
gh pr create --title "..." --body "..."

# Cleanup
cd /home/azureuser/src/MicrosoftHackathon2025-AgenticCoding
git worktree remove /tmp/worktree-issue-{N}
```

**2. Cherry-Pick Conflict Resolution:**

```bash
# When branches diverge from main
git fetch origin main
git cherry-pick origin/main

# Resolve conflicts systematically
/fix import   # Dependency issues
/fix config   # Configuration updates
/fix quality  # Formatting and style
```

**3. Message Reduction Features (High Value):**

- Budget awareness warnings (prevent token exhaustion)
- Complexity estimator (right-size responses)
- Message consolidation (reduce API calls)
- Context prioritization (focus on relevant info)
- Summary generation (compress long threads)
- Progressive disclosure (hide verbose output)
- Smart truncation (preserve key information)

### Key Learnings

1. **Parallel Agent Execution is Highly Effective**
   - Independent tasks can run simultaneously without interference
   - Worktrees provide perfect isolation mechanism
   - No performance degradation with 13 parallel workflows
   - Agent specialization maintains quality at scale

2. **Fix-Agent Pattern Scales Well**
   - Template-based resolution handles common patterns
   - Cherry-pick strategy effective for divergent branches
   - Pattern recognition accelerates conflict resolution
   - Systematic approach prevents mistakes under pressure

3. **Documentation-First is Lightweight and Effective**
   - Templates reduce decision overhead
   - Workflows provide clear process guidance
   - Documentation faster than code generation
   - Reusable across multiple features

4. **Message Reduction Features Address Real Pain Points**
   - Token budget exhaustion is frequent blocker
   - Response complexity often mismatched to need
   - API call volume impacts performance
   - Users need more control over verbosity

5. **Philosophy Compliance Remains High at Scale**
   - All 13 PRs scored 9-10/10
   - Ruthless simplicity maintained
   - Zero-BS implementation enforced
   - Modular design preserved

6. **Reflection System Generates Actionable Insights**
   - Identified 13 concrete improvement opportunities
   - Each mapped to specific user pain points
   - Clear implementation paths
   - Measurable impact potential

### Impact

**Demonstrates System Scalability:**

- Workflow handles 13+ concurrent tasks without degradation
- Agent orchestration remains effective at scale
- Quality standards maintained across all implementations
- Process is repeatable and systematic

**Validates Architecture Decisions:**

- Worktree isolation strategy proven
- Agent specialization approach validated
- Fix-agent pattern confirmed effective
- Documentation-first approach successful

**Identifies Improvement Opportunities:**

- Message reduction features fill real gaps
- Token budget management needs better tooling
- Response complexity should be tunable
- API call optimization has significant value

### Prevention Patterns

**For Future Parallel Execution:**

1. **Always Use Worktrees for Parallel Work**
   - One worktree per feature/issue
   - Prevents branch interference
   - Enables true parallel development
   - Easy cleanup with `git worktree remove`

2. **Cherry-Pick for Divergent Branches**
   - When branches diverge from main
   - Systematic conflict resolution
   - Preserves both lineages
   - Better than rebase for parallel work

3. **Use Fix-Agent for Systematic Resolution**
   - Pattern-based conflict handling
   - Template-driven solutions
   - Quick mode for common issues
   - Diagnostic mode for complex problems

4. **Document Templates Before Mass Changes**
   - Create reusable message templates
   - Document fix patterns
   - Write workflow guides
   - Templates save time at scale

### Files Modified/Created

**Issues Created:**

- #1089: Budget awareness warnings
- #1090: Message complexity estimator
- #1091: Message consolidation
- #1092: Context prioritization
- #1093: Summary generation
- #1094: Progressive disclosure
- #1095: Smart truncation
- #1096-#1101: Additional improvements

**PRs Created (all with 9-10/10 philosophy compliance):**

- PR #1102-#1114: Feature implementations

**Templates Created:**

- `.claude/data/message_templates/` (various)
- `.claude/data/fix_templates/` (import, config, quality, etc.)

**Workflows Documented:**

- `docs/workflows/parallel_execution.md`
- `docs/workflows/worktree_management.md`
- `docs/workflows/conflict_resolution.md`

### Related Patterns

**Enhances Existing Patterns:**

- Microsoft Amplifier Parallel Execution Engine (CLAUDE.md)
- Parallel execution templates and protocols
- Agent delegation strategy
- Fix-agent workflow optimization

**Validates Philosophy Principles:**

- Ruthless Simplicity: Maintained at scale
- Modular Design: Bricks & studs approach works
- Zero-BS Implementation: No shortcuts taken
- Agent Delegation: Orchestration over implementation

### Recommendations

1. **Promote Worktree Pattern as Standard**
   - Default approach for feature work
   - Document in onboarding materials
   - Add to workflow templates
   - Create helper scripts for common operations

2. **Expand Fix-Agent Template Library**
   - Add more common patterns
   - Document resolution strategies
   - Create decision trees for pattern selection
   - Measure effectiveness metrics

3. **Prioritize Message Reduction Features**
   - High user value (budget, complexity, consolidation)
   - Clear implementation paths
   - Measurable impact
   - Address frequent pain points

4. **Create Parallel Execution Playbook**
   - Document lessons learned
   - Provide concrete examples
   - Include troubleshooting guide
   - Share best practices

### Verification

**All 13 Workflows Completed Successfully:**

- âœ… Issues created with clear requirements
- âœ… Branches created with descriptive names
- âœ… Code implemented following specifications
- âœ… Tests passing (where applicable)
- âœ… Philosophy compliance 9-10/10
- âœ… PRs created with complete documentation
- âœ… No cross-contamination between features
- âœ… Systematic conflict resolution applied

**Performance Metrics:**

- Total time: ~4 hours for 13 features
- Average per feature: ~18 minutes
- Philosophy compliance: 9-10/10 average
- Success rate: 100%

### Next Steps

1. **Review PRs and merge successful implementations**
2. **Extract reusable patterns into documentation**
3. **Update workflow with lessons learned**
4. **Create templates for future parallel execution**
5. **Document worktree management best practices**
6. **Expand fix-agent template library**
7. **Implement high-priority message reduction features**

---

<!-- New discoveries will be added here as the project progresses -->

## SessionStart and Stop Hooks Executing Twice - Claude Code Bug (2025-11-21)

### Discovery

SessionStart and Stop hooks are executing **twice per session** due to a **known Claude Code bug in the hook execution engine** (#10871), NOT due to configuration errors. The issue affects all hook types and causes performance degradation and duplicate context injection.

### Context

Investigation triggered by system reminder messages showing "SessionStart:startup hook success: Success" appearing twice. Initial hypothesis was incorrect configuration format, but deeper analysis revealed the configuration is correct per official schema.

### Root Cause

**Claude Code Internal Bug**: The hook execution engine spawns **two separate Python processes** for each hook invocation, regardless of configuration.

**Current Configuration** (CORRECT per schema):

```json
"SessionStart": [
  {
    "hooks": [  // âœ“ Required by Claude Code schema
      {
        "type": "command",
        "command": "$CLAUDE_PROJECT_DIR/.claude/tools/amplihack/hooks/session_start.py",
        "timeout": 10000
      }
    ]
  }
]
```

**Schema Requirement**:

```typescript
{
  "required": ["hooks"],  // The "hooks" wrapper is MANDATORY
  "additionalProperties": false
}
```

### Initial Hypothesis Was Wrong

**Initial theory**: Extra `"hooks": []` wrapper was causing duplication.

**Reality**: The wrapper is **required by Claude Code schema**. Removing it causes validation errors:

```
Settings validation failed:
- hooks.SessionStart.0.hooks: Expected array, but received undefined
```

**Actual cause**: Claude Code's hook execution engine has an internal bug that spawns two separate processes for each registered hook.

### Evidence

**Configuration Analysis**:

- Only 1 SessionStart hook registered in settings.json
- No duplicate configurations found
- Schema validation confirms format is correct
- **Two separate Python processes** spawn anyway (different PIDs)

**From `.claude/runtime/logs/session_start.log`**:

```
[2025-11-21T13:01:07.113446] INFO: session_start hook starting (Python 3.13.9)
[2025-11-21T13:01:07.113687] INFO: session_start hook starting (Python 3.13.9)
```

**From `.claude/runtime/logs/stop.log`**:

```
[2025-11-20T21:37:05.173846] INFO: stop hook starting (Python 3.13.9)
[2025-11-20T21:37:05.427256] INFO: stop hook starting (Python 3.13.9)
```

**Pattern**: All hooks (SessionStart, Stop, PostToolUse) show double execution with microsecond-level timing differences, indicating true parallel process spawning.

### Impact

| Area                  | Effect                                                   |
| --------------------- | -------------------------------------------------------- |
| **Performance**       | 2-4 seconds wasted per session (double process spawning) |
| **Context Pollution** | USER_PREFERENCES.md injected twice (~19KB duplicate)     |
| **Side Effects**      | File writes, metrics, logs all duplicated                |
| **Log Clarity**       | Every entry appears twice, making debugging confusing    |
| **Resource Usage**    | Double memory allocation, double I/O operations          |

### Solution

**NO CODE FIX AVAILABLE** - This is a Claude Code internal bug.

**Workarounds**:

1. Accept the duplication (hooks are idempotent, safe but wasteful)
2. Add process-level deduplication in hook_processor.py (complex)
3. Wait for upstream Claude Code fix

**Tracking**: Claude Code GitHub Issue #10871 "Plugin-registered hooks are executed twice with different PIDs"

### Configuration Format (CORRECT)

Our configuration **matches the official schema exactly**:

```json
"SessionStart": [
  {
    "hooks": [  // âœ“ REQUIRED by schema
      {
        "type": "command",
        "command": "$CLAUDE_PROJECT_DIR/.claude/tools/amplihack/hooks/session_start.py",
        "timeout": 10000
      }
    ]
  }
]
```

**Schema requirement**:

```typescript
"required": ["hooks"],  // The "hooks" wrapper is MANDATORY
"additionalProperties": false
```

Attempting to remove the wrapper causes validation errors.

### Affected Hooks

| Hook             | Status     | Root Cause             |
| ---------------- | ---------- | ---------------------- |
| **SessionStart** | âŒ Runs 2x | Claude Code bug #10871 |
| **Stop**         | âŒ Runs 2x | Claude Code bug #10871 |
| **PostToolUse**  | âŒ Runs 2x | Claude Code bug #10871 |
| PreToolUse       | â“ Unknown | Likely affected        |
| PreCompact       | â“ Unknown | Likely affected        |

### Key Learnings

1. **Configuration was correct all along** - The `"hooks": []` wrapper is required by Claude Code schema
2. **Schema validation prevents incorrect "fixes"** - Attempted to remove wrapper, got validation errors
3. **Log analysis reveals issues but not always root cause** - Duplicate execution doesn't always mean duplicate configuration
4. **Upstream bugs affect downstream projects** - Known Claude Code bug (#10871) causes systematic duplication
5. **Idempotent design saves us** - Hooks are safe to run twice even though wasteful
6. **Investigation workflow worked** - Systematic analysis prevented incorrect fix from being deployed

### No Action Required

**Decision**: Accept the duplication as a known limitation until Claude Code team fixes #10871.

**Rationale**:

- Configuration is correct per official schema
- No user-side fix available without breaking schema validation
- Hooks are idempotent (safe to run twice)
- Performance impact acceptable (~2 seconds per session)
- Workarounds (process-level dedup) would add significant complexity

### Monitoring

Track Claude Code GitHub for fix:

- **Issue #10871**: "Plugin-registered hooks are executed twice with different PIDs"
- **Related**: #3523 (hook duplication), #3465 (hooks fired twice from home dir)

### Verification

Configuration correctness verified:

1. âœ… Only 1 hook registered per event type
2. âœ… Schema validation passes
3. âœ… Format matches official Claude Code documentation
4. âœ… Removing wrapper causes validation errors
5. âœ… Both processes run to completion (not a race condition)

### Files Analyzed

- `.claude/settings.json` (1 SessionStart hook, 1 Stop hook)
- `.claude/tools/amplihack/hooks/session_start.py` (hook implementation)
- `.claude/runtime/logs/session_start.log` (execution evidence)
- `.claude/runtime/logs/stop.log` (execution evidence)
- Claude Code schema (hook format requirements)

---

## Remember

- Document immediately while context is fresh
- Include specific error messages and stack traces
- Show actual code that fixed the problem
- Think about broader implications
- Update PATTERNS.md when a discovery becomes a reusable pattern

---

## Expert Agent Creation Pattern from Knowledge Bases (2025-10-18)

### Discovery

Successfully established reusable pattern for creating domain expert agents grounded in focused knowledge bases, achieving 10-20x learning speedup over traditional methods.

### Context

After merging PR #931 (knowledge-builder refactoring), tested end-to-end workflow by creating two expert agents:

1. Rust Programming Expert (memory safety, ownership)
2. Azure Kubernetes Expert (production AKS deployments)

### Pattern Components

**1. Focused Knowledge Base Structure**

```
.claude/data/{domain_name}/
â”œâ”€â”€ Knowledge.md          # 7-10 core concepts with Q&A
â”œâ”€â”€ KeyInfo.md           # Executive summary, learning path
â””â”€â”€ HowToUseTheseFiles.md # Usage patterns, scenarios
```

**2. Knowledge Base Content**

- Q&A format (not documentation style)
- 2-3 practical code examples per concept
- Actionable, not theoretical
- Focused on specific use case (not 270 generic questions)

**3. Expert Agent Definition**

```markdown
---
description: {Domain} expert with...
knowledge_base: .claude/data/{domain_name}/
priority: high
---

# {Domain} Expert Agent

[References knowledge base, defines competencies, usage patterns]
```

### Key Learnings

1. **Focused Beats Breadth**
   - 7 focused concepts > 270 generic questions
   - Evidence: Rust implementation in 2 hours vs 20-40 hour traditional learning
   - Result: 10-20x speedup for project-specific domains

2. **Q&A Format Superior to Documentation**
   - Natural learning progression
   - "Why" alongside "how"
   - Easy to reference during implementation
   - Agent scored 9.5/10 in evaluation

3. **Real Code Examples Essential**
   - Working examples 10x more valuable than explanations
   - Can copy/adapt directly into implementation
   - Every concept needs 2-3 runnable examples

4. **Performance Matters for Adoption**
   - 30-minute generation time blocks practical use
   - Focused manual creation: 20 minutes
   - **Recommendation**: Add `--depth` parameter (shallow/medium/deep)

### Files Created

**Expert Agents:**

- `.claude/agents/amplihack/specialized/rust-programming-expert.md` (156 lines)
- `.claude/agents/amplihack/specialized/azure-kubernetes-expert.md` (262 lines)

**Rust Knowledge Base:**

- `amplihack-logparse/.claude/data/rust_focused_for_log_parser/Knowledge.md` (218 lines)
- `amplihack-logparse/.claude/data/rust_focused_for_log_parser/KeyInfo.md` (67 lines)
- `amplihack-logparse/.claude/data/rust_focused_for_log_parser/HowToUseTheseFiles.md` (83 lines)

**Azure AKS Knowledge Base:**

- `.claude/data/azure_aks_expert/Knowledge.md` (986 lines, 30+ examples)
- `.claude/data/azure_aks_expert/KeyInfo.md` (172 lines)
- `.claude/data/azure_aks_expert/HowToUseTheseFiles.md` (275 lines)

**Rust Log Parser (demonstrating knowledge application):**

- `amplihack-logparse/src/types.rs` (91 lines) - Ownership
- `amplihack-logparse/src/error.rs` (62 lines) - Error handling
- `amplihack-logparse/src/parser/mod.rs` (165 lines) - Borrowing, Result
- `amplihack-logparse/src/analyzer/mod.rs` (673 lines) - Traits
- `amplihack-logparse/src/main.rs` (wired up CLI)
- **Test Status**: 24/24 tests passing

### Verification

**Rust Expert Agent Test:**

- Question: Borrow checker lifetime error
- Result: Correctly referenced Lifetimes section (Knowledge.md lines 52-72)
- Provided: Proper fix with lifetime annotations
- Score: 9.5/10

**Azure AKS Expert Agent Test:**

- Question: Production deployment with HTTPS, autoscaling, Key Vault, monitoring
- Result: Correctly referenced 4 knowledge base sections
- Provided: Complete Azure CLI commands and YAML manifests
- Score: PASS (production-ready)

### Recommendations

1. **Optimize knowledge-builder performance**

   ```bash
   /knowledge-builder "topic" --depth shallow    # 10 questions, 2-3 min
   /knowledge-builder "topic" --depth medium     # 30 questions, 5-10 min
   /knowledge-builder "topic" --depth deep       # 270 questions, 30+ min
   ```

2. **Add focus parameter**

   ```bash
   /knowledge-builder "Rust" --focus "ownership,borrowing"
   ```

3. **Create more domain experts using this pattern**
   - AWS EKS (similar to AKS)
   - Terraform (infrastructure as code)
   - PostgreSQL (database operations)
   - React + TypeScript (frontend development)

### Impact

**Pattern Reusability**: Can be applied to any technical domain
**Learning Speedup**: 10-20x faster for project-specific learning
**Agent Quality**: Both agents production-ready, comprehensively tested
**Cost-Benefit**: ~1 hour per agent after pattern established

### Related Issues/PRs

- **Issue**: #930
- **PR**: #931 (knowledge-builder refactoring, MERGED)
- **PR**: #941 (auto mode fix, MERGED)

---

## Neo4j Container Port Mismatch Detection Bug (2025-11-08)

### Issue

Amplihack startup would fail with container name conflicts when starting in a different project directory than where the Neo4j container was originally created, even though a container with the expected name already existed:

```
âœ… Our Neo4j container found on ports 7787/7774
Query failed... localhost:7688 (Connection refused)
Failed to create container... Conflict... already in use
```

### Root Cause

**Logic Flaw in Port Detection**: The `is_our_neo4j_container()` function checked if a container with the expected NAME existed, but didn't retrieve the ACTUAL ports the container was using.

**Exact Bug Location**: `src/amplihack/memory/neo4j/port_manager.py:147-149`

```python
# BROKEN - Assumes container is on ports from .env
if is_our_neo4j_container():  # Only checks name, doesn't get ports!
    messages.append(f"âœ… Our Neo4j container found on ports {bolt_port}/{http_port}")
    return bolt_port, http_port, messages  # Returns WRONG ports from .env
```

**Error Sequence**:

1. Container exists on ports 7787/7774 (actual)
2. `.env` in new directory has port 7688 (wrong)
3. Code detects container exists by name âœ…
4. Code assumes container is on 7688 (from `.env`) âŒ
5. Connection to 7688 fails (nothing listening)
6. Code tries to create new container (name conflict)

### Solution

**Added `get_container_ports()` function** that queries actual container ports using `docker port`:

```python
def get_container_ports(container_name: str = "amplihack-neo4j") -> Optional[Tuple[int, int]]:
    """Get actual ports from running Neo4j container.

    Uses `docker port` command to inspect actual port mappings,
    not what .env file claims.

    Returns:
        (bolt_port, http_port) if container running with ports, None otherwise
    """
    result = subprocess.run(
        ["docker", "port", container_name],
        capture_output=True,
        timeout=5,
        text=True,
    )

    if result.returncode != 0:
        return None

    # Parse output: "7687/tcp -> 0.0.0.0:7787"
    # Extract actual host ports from both ports
    # ...
    return bolt_port, http_port
```

**Updated `resolve_port_conflicts()`** to use actual ports:

```python
# FIXED - Use actual container ports, not .env ports
container_ports = get_container_ports("amplihack-neo4j")
if container_ports:
    actual_bolt, actual_http = container_ports
    messages.append(f"âœ… Our Neo4j container found on ports {actual_bolt}/{actual_http}")

    # Update .env if ports don't match
    if (actual_bolt != bolt_port or actual_http != http_port) and project_root:
        _update_env_ports(project_root, actual_bolt, actual_http)
        messages.append(f"âœ… Updated .env to match container ports")

    return actual_bolt, actual_http, messages
```

### Key Learnings

1. **Container Detection â‰  Port Detection** - Knowing a container exists doesn't tell you what ports it's using
2. **`.env` Files Can Lie** - Configuration files can become stale, always verify actual runtime state
3. **Docker Port Command is Canonical** - `docker port <container>` returns actual mappings, not configured values
4. **Self-Healing Behavior** - Automatically updating `.env` to match reality prevents future failures
5. **Challenge User Assumptions** - The user was right that stopping the container wasn't the real fix - the port mismatch was the actual issue

### Prevention

**Before this fix:**

- Starting amplihack in multiple directories would fail with container conflicts
- Users had to manually sync `.env` files across projects
- No automatic detection of port mismatches

**After this fix:**

- Amplihack automatically detects actual container ports
- `.env` files auto-update to match reality
- Can start amplihack in any directory, will reuse existing container
- Self-healing behavior prevents stale configuration issues

### Testing

**Comprehensive test coverage (29 tests, all passing)**:

- Docker port output parsing (12 tests)
- Port conflict resolution (5 tests)
- Port availability detection (4 tests)
- Edge cases (5 tests)
- Integration scenarios (3 tests)

**Test Location**: `tests/unit/memory/neo4j/test_port_manager.py`

### Files Modified

- `src/amplihack/memory/neo4j/port_manager.py`: Added `get_container_ports()`, updated `resolve_port_conflicts()`
- `tests/unit/memory/neo4j/test_port_manager.py`: Added comprehensive test suite (29 tests)

### Verification

**Original Error Reproduced**: âœ…
**Fix Applied**: âœ…
**All Tests Passing**: âœ… 29/29
**Self-Healing Confirmed**: âœ… `.env` updates automatically

### Pattern Recognition

**Trigger Signs of Port Mismatch Issues**:

- "Container found" but connection fails
- "Conflict" errors when creating containers
- Port numbers in error messages don't match expected ports
- Working in different directories with shared container

**Debugging Approach**:

1. Check if container actually exists (`docker ps`)
2. Check what ports container is actually using (`docker port <name>`)
3. Check what ports configuration expects (`.env`, config files)
4. Fix: Use actual ports, not configured ports

### Philosophy Alignment

- **Ruthless Simplicity**: Single function solves the problem, minimal changes
- **Self-Healing**: System automatically corrects stale configuration
- **Zero-BS**: No workarounds, addresses root cause directly
- **Reality Over Configuration**: Trust Docker's actual state, not config files

---

## Power Steering Mode Branch Divergence (2025-11-16)

### Problem

Power steering feature not activating - appeared disabled.

### Root Cause

**Feature was missing from branch entirely**. Branch diverged from main BEFORE power steering was merged.

### Solution

Sync branch with main: `git rebase origin/main`

### Key Learnings

"Feature not working" can mean "Feature not present". Always check git history: `git log HEAD...origin/main`

---

## Mandatory End-to-End Testing Pattern (2025-11-10)

### Problem

Code committed after unit tests and reviews but missing real user experience validation.

### Solution

**ALWAYS test with `uvx --from <branch>` before committing**:

```bash
uvx --from git+https://github.com/org/repo@branch package command
```

This verifies: package installation, dependency resolution, actual user workflow, error messages, config updates.

### Key Learnings

Testing hierarchy (all required):

1. Unit tests
2. Integration tests
3. Code reviews
4. **End-to-end user experience test** (MANDATORY BEFORE COMMIT)

---

## Neo4j Container Port Mismatch Bug (2025-11-08)

### Problem

Startup fails with container conflicts when starting in different directory than where Neo4j container was created.

### Root Cause

`is_our_neo4j_container()` checked container NAME but not ACTUAL ports. `.env` can become stale.

### Solution

Added `get_container_ports()` using `docker port` to query actual ports. Auto-update `.env` to match reality.

### Key Learnings

Container Detection != Port Detection. `.env` files can lie. Docker port command is canonical.

---

## Parallel Reflection Workstream Execution (2025-11-05)

### Context

Successfully executed 13 parallel full-workflow tasks simultaneously using worktree isolation.

### Key Metrics

- 13 issues created (#1089-#1101)
- 13 PRs with 9-10/10 philosophy compliance
- 100% success rate
- ~18 minutes per feature average

### Patterns That Worked

1. **Worktree Isolation**: Each feature in separate worktree
2. **Agent Specialization**: prompt-writer â†’ architect â†’ builder â†’ reviewer
3. **Cherry-Pick for Divergent Branches**: Better than rebase for parallel work
4. **Documentation-First**: Templates reduce decision overhead

### Key Learnings

Parallel execution scales well. Worktrees provide perfect isolation. Philosophy compliance maintained at scale.

---

## Pattern Applicability Analysis Framework (2025-10-20)

### Context

Evaluated PBZFT vs N-Version Programming. PBZFT would be 6-9x more complex with zero benefit.

### Six Meta-Patterns Identified

1. **Threat Model Precision**: Match defense to actual failure mode
2. **Voting vs Expert Judgment**: Expert review for quality, voting for adversarial consensus
3. **Distributed Systems Applicability Test**: Most patterns don't apply to AI (different trust model)
4. **Complexity-Benefit Ratio**: Require >3.0 ratio to justify complexity
5. **Domain Appropriateness Check**: Best practices are domain-specific
6. **Diversity as Error Reduction**: Independent implementations reduce correlated errors

### Key Learnings

- Threat model mismatch is primary source of inappropriate pattern adoption
- Distributed systems patterns rarely map to AI systems
- Always verify failure modes match before importing patterns

**Note**: Consider promoting to PATTERNS.md if framework used 3+ times.

---

## Socratic Questioning Pattern (2025-10-18)

### Context

Developed effective method for deep, probing questions in knowledge-builder scenarios.

### Three-Dimensional Attack Strategy

1. **Empirical**: Challenge with observable evidence
2. **Computational**: Probe tractability and complexity
3. **Formal Mathematical**: Demand precise relationships

### Usage Context

- When: Knowledge exploration, challenging claims, surfacing assumptions
- When NOT: Simple factual questions, time-sensitive decisions

**Status**: 1 successful usage. Needs 2-3 more before promoting to PATTERNS.md.

---

## Expert Agent Creation Pattern (2025-10-18)

### Context

Created Rust and Azure Kubernetes expert agents with 10-20x learning speedup.

### Pattern Components

1. **Focused Knowledge Base**: 7-10 core concepts in Q&A format
2. **Structure**: `Knowledge.md`, `KeyInfo.md`, `HowToUseTheseFiles.md`
3. **Expert Agent**: References knowledge base, defines competencies

### Key Learnings

- Focused beats breadth (7 concepts > 270 generic questions)
- Q&A format superior to documentation style
- Real code examples are essential (2-3 per concept)

**Note**: Consider promoting to PATTERNS.md if used 3+ times.

---

## Remember

- Document immediately while context is fresh
- Include specific error messages
- Show code that fixed the problem
- Update PATTERNS.md when a discovery becomes reusable
- Archive entries older than 3 months to DISCOVERIES_ARCHIVE.md

## 2025-12-01: STOP Gates Break Sonnet, Help Opus - Model-Specific Prompt Behavior (Issue #1755)

**Context**: Testing CLAUDE.md modifications across both Opus and Sonnet models revealed same text produces opposite outcomes.

**Problem**: STOP validation gates have model-specific effects:

- **Opus 4.5**: STOP gates help (20/22 â†’ 22/22 steps) âœ…
- **Sonnet 4.5**: STOP gates break (22/22 â†’ 8/22 steps) âŒ
- **Root cause**: Different models interpret validation language differently

**Solution**: V2 (No STOP Gates) - Remove validation checkpoints while keeping workflow structure

**Results** (6/8 benchmarks complete, 75%):

Sonnet V2:

- âœ… MEDIUM: 24.8m, $5.47, 22/22 steps (-16% cost improvement)
- âœ… HIGH: 21.7m, $4.92, 22 turns (-12% duration vs MEDIUM - negative scaling!)

Opus V2:

- âœ… MEDIUM: 61.5m, $56.86, ~20/22 steps (-12% duration, -21% cost improvement!)
- â³ HIGH: Testing (~4.5 hours remaining)

**Key Insights**:

1. **Multi-Model Testing Required**: Same prompt can help one model while breaking another
2. **STOP Gate Paradox**: Removing validation gates IMPROVES performance (12-21% cost reduction)
3. **Negative Complexity Scaling**: V2 HIGH faster than MEDIUM for well-defined tasks (task clarity > complexity)
4. **Universal Optimization**: V2 improves BOTH models, not just fixes one
5. **High-Salience Language Risky**: "STOP", "MUST", ALL CAPS trigger different model responses

**Impact**:

- Fixes Sonnet degradation completely (8/22 â†’ 22/22)
- Improves Sonnet performance (-12% to -16%)
- Improves Opus performance (-12% to -21%)
- $20K-$406K annual savings (moderate: $81K/year)
- Universal solution (single CLAUDE.md for both models)

**Implementation**: V2 deployed when Opus HIGH validates (expected)

**Related**: #1755, #1703, #1687

**Pattern Identified**: Validation checkpoints can backfire - use flow language instead of interruption language

**Lesson**: Always validate AI guidance changes empirically with ALL target models before deploying

---

## Mandatory User Testing Validates Its Own Value {#mandatory-user-testing-validates-value-2025-12-02}

**Date**: 2025-12-02
**Context**: Implementing Parallel Task Orchestrator (Issue #1783, PR #1784)
**Impact**: HIGH - Validates mandatory testing requirement, found production-blocking bug

### Problem

Unit tests can achieve high coverage (86%) and 100% pass rate while missing critical real-world bugs.

### Discovery

Mandatory user testing (USER_PREFERENCES.md requirement) caught a **production-blocking bug** that 110 passing unit tests missed:

**Bug**: `SubIssue` dataclass not hashable, but `OrchestrationConfig` uses `set()` for deduplication

```python
# This passed all unit tests but fails in real usage:
config = OrchestrationConfig(sub_issues=[...])
# TypeError: unhashable type: 'SubIssue'
```

### How It Was Missed

**Unit Tests** (110/110 passing):

- Mocked all `SubIssue` creation
- Never tested real deduplication path
- Assumed API worked without instantiation

**User Testing** (mandatory requirement):

- Tried actual config creation
- **Bug discovered in <2 minutes**
- Immediate TypeError on first real use

### Fix

```python
# Before
@dataclass
class SubIssue:
    labels: List[str] = field(default_factory=list)

# After
@dataclass(frozen=True)
class SubIssue:
    labels: tuple = field(default_factory=tuple)
```

### Validation

**Test Results After Fix**:

```
âœ… Config creation works
âœ… Deduplication works (3 items â†’ 2 unique)
âœ… Orchestrator instantiation works
âœ… Status API functional
```

### Key Insights

1. **High test coverage â‰  Real-world readiness**
   - 86% coverage, 110/110 tests, still had production blocker
   - Mocks hide integration issues

2. **User testing finds different bugs**
   - Unit tests validate component logic
   - User tests validate actual workflows
   - Both are necessary

3. **Mandatory requirement justified**
   - Without user testing, would've shipped broken code
   - CI wouldn't catch this (unit tests pass)
   - First user would've hit TypeError

4. **Time investment worthwhile**
   - <5 minutes of user testing
   - Found bug that could've cost hours of debugging
   - Prevented embarrassing production failure

### Implementation

**Mandatory User Testing Pattern**:

```bash
# Test like a user would
python -c "from module import Class; obj = Class(...)"  # Real instantiation
config = RealConfig(real_data)  # No mocks
result = api.actual_method()  # Real workflow
```

**NOT sufficient**:

```python
# Unit test approach (can miss real issues)
@patch("module.Class")
def test_with_mock(mock_class):  # Never tests real instantiation
    ...
```

### Lessons Learned

1. **Always test like a user** - No mocks, real instantiation, actual workflows
2. **High coverage isn't enough** - Need real usage validation
3. **Mocks hide bugs** - Integration issues invisible to mocked tests
4. **User requirements are wise** - This explicit requirement saved us from shipping broken code

### Related

- Issue #1783: Parallel Task Orchestrator
- PR #1784: Implementation
- USER_PREFERENCES.md: Mandatory E2E testing requirement
- Commit dc90b350: Hashability fix

### Recommendation

**ENFORCE mandatory user testing** for ALL features:

- Test with `uvx --from git+...` (no local state)
- Try actual user workflows (no mocks)
- Verify error messages and UX
- Document test results in PR

This discovery **validates the user's explicit requirement** - mandatory user testing prevents production failures that unit tests miss.

## Discovery: GitHub Pages MkDocs Deployment Requires docs/.claude/ Copy

**Date**: 2025-12-02
**Issue**: #1827
**PR**: #1829

**Context**: GitHub Pages documentation deployment was failing with 133 mkdocs warnings and 305 total broken links. The mkdocs build couldn't find `.claude/` content referenced in navigation.

**Problem**: MkDocs expects all content in `docs/` directory, but our `.claude/` directory (containing agents, workflows, commands, skills) was at project root. Navigation links to `.claude/` files resulted in 404s.

**Solution**: Copy entire `.claude/` structure to `docs/.claude/` (776 files)

**Why This Works**:

- MkDocs site_dir scans `docs/` by default
- All navigation references now resolve correctly
- Cross-references between docs preserved
- No complex symlinks or build scripts needed

**Implementation**:

```bash
# Copy .claude/ to docs/.claude/
cp -r .claude docs/.claude

# Update mkdocs.yml navigation to reference docs/.claude/ paths
# Example: '.claude/agents/architect.md' works in navigation
```

**Impact**:

- âœ… mkdocs build succeeds (was failing with 133 warnings)
- âœ… GitHub Pages deployment unblocked
- âœ… All framework documentation accessible in docs site
- âœ… 305 broken links resolved

**Trade-offs**:

- **Pros**: Ruthlessly simple, no build complexity, works immediately
- **Cons**: Duplicates `.claude/` content (+776 files in docs/), increases repo size by ~1MB

**Philosophy Alignment**: âœ… Ruthless Simplicity

- Avoided complex symlink solutions
- No custom build scripts needed
- Zero-BS implementation (everything works)
- Modular (can be regenerated easily)

**Alternatives Considered**:

1. **Symlinks**: Would break on Windows, adds complexity
2. **Build script**: Adds build-time dependency, complexity
3. **Git submodules**: Overkill, adds workflow friction
4. **Custom MkDocs plugin**: Over-engineering for simple problem

**Lessons Learned**:

1. MkDocs `docs/` directory is the source of truth - work with it, not against it
2. File duplication is acceptable when it eliminates build complexity
3. For documentation systems, **copying > symlinking** for portability
4. Always test mkdocs build locally before pushing docs changes

**Prevention**:

- Add `mkdocs build --strict` to CI/GitHub Actions
- Catches broken navigation before deployment
- Test with: `mkdocs build && mkdocs serve` locally

**Related Patterns**:

- Ruthless Simplicity (PHILOSOPHY.md)
- Zero-BS Implementation (PATTERNS.md)

**Tags**: #documentation #mkdocs #github-pages #deployment #simplicity
