# DISCOVERIES.md

This file documents non-obvious problems, solutions, and patterns discovered
during development. It serves as a living knowledge base that grows with the
project.

## Ubuntu Version Update Available - 24.04 LTS Supported (2026-01-17)

### Investigation Summary

Investigated current Ubuntu version used by azlin (22.04) and discovered that Azure now supports **Ubuntu 24.04 LTS (Noble Numbat)**, which is significantly newer and recommended for production use.

### Key Findings

**Current State**: azlin uses Ubuntu 22.04 (Jammy Jellyfish) in three locations
**Available**: Ubuntu 24.04 LTS (Noble Numbat) - latest LTS, released April 2024
**Compatibility**: All post-launch setup scripts (cloud-init) compatible with 24.04

#### Ubuntu Version Locations

Found three hardcoded references to Ubuntu 22.04:

1. **Terraform Strategy** (`src/azlin/agentic/strategies/terraform_strategy.py:481-486`)
   - Current: `offer = "0001-com-ubuntu-server-jammy"`, `sku = "22_04-lts-gen2"`
   - Update to: `offer = "ubuntu-24_04-lts"`, `sku = "server"`

2. **Azure CLI Strategy** (`src/azlin/agentic/strategies/azure_cli.py:413, 445`)
   - Current: `--image Ubuntu2204`
   - Update to: `--image Canonical:ubuntu-24_04-lts:server:latest`

3. **Sample Config/VM Provisioning** (`tests/fixtures/sample_configs.py:22, 119, 174` and `src/azlin/vm_provisioning.py:43`)
   - Current: `"image": "ubuntu-22.04"` and `image: str = "Ubuntu2204"`
   - Update to: `"image": "ubuntu-24.04"` and `image: str = "Ubuntu2404"`

#### Azure Image Details

Ubuntu 24.04 LTS available via:
- **Full URN**: `Canonical:ubuntu-24_04-lts:server:latest`
- **SKUs available**: server, minimal, server-arm64, cvm, ubuntu-pro
- **Recommendation**: Use `server` SKU for standard Gen2 VMs

#### Post-Launch Setup Compatibility Analysis

Reviewed cloud-init script in `vm_provisioning.py:702-799`:
- ✅ All package managers compatible (apt, snap, ppa)
- ✅ All packages available on Ubuntu 24.04
- ✅ Python 3.13 from deadsnakes PPA - works on 24.04
- ✅ GitHub CLI - architecture-independent
- ✅ Azure CLI - supports Ubuntu 24.04
- ✅ Node.js 20.x - supports Ubuntu 24.04
- ✅ Docker - available on 24.04

**Result**: No compatibility issues identified. All setup should work seamlessly.

### Recommendations

1. **Update default image to Ubuntu 24.04 LTS** in all three locations
2. **Test on single VM** before updating defaults
3. **Update documentation** to reflect new Ubuntu version
4. **Consider adding image parameter** to allow users to choose version

### Supporting Evidence

Azure CLI verification:
```bash
$ az vm image list --publisher Canonical --offer ubuntu-24_04-lts --sku server --all
# Returns multiple versions from 24.04.202404230 onwards
```

Available Ubuntu versions in Azure:
- Ubuntu 24.04 LTS (Noble) - **RECOMMENDED** - Latest LTS
- Ubuntu 23.10 (Mantic) - Available but short support
- Ubuntu 23.04 (Lunar) - EOL, not recommended
- Ubuntu 22.04 LTS (Jammy) - **CURRENT** - Still supported but older

### Lessons Learned

1. **Azure image naming conventions changed** - Old format: `0001-com-ubuntu-server-jammy`, New format: `ubuntu-24_04-lts`
2. **Always check cloud provider for newer LTS releases** - 24.04 has been available since April 2024
3. **Cloud-init scripts using standard package managers** are highly portable across Ubuntu versions

## Transcripts System Investigation - Architecture Validated, Microsoft Amplifier Comparison Complete (2025-11-22)

### Investigation Summary

Conducted comprehensive investigation of amplihack's transcript system architecture, validated against documentation, and compared with Microsoft Amplifier's approach. **Key finding: amplihack's independent, philosophy-aligned architecture is superior for its use case.**

### Key Findings

**Decision**: Maintain current transcript system architecture
**Rationale**: Perfect philosophy alignment (30/30) + proven stability + zero user demand for alternatives

#### Architecture Validation

amplihack transcript system uses **2-tier builder pattern**:

- **ClaudeTranscriptBuilder** (596 lines) - Raw data capture from hooks
- **CodexTranscriptsBuilder** (769 lines) - Knowledge extraction and Markdown generation

**4 Strategic Hooks**: SessionStart, PostToolUse, PreCompact, Stop

**Session-Isolated Storage**: `.claude/runtime/logs/{session_id}/` with JSON + Markdown formats

**Philosophy Score**: 30/30 (perfect alignment)

#### Documentation Validation Results

**Documentation accuracy**: 100% - All verified correct

- ✅ All path references correctly use `.claude/runtime/logs/`
- ✅ Architecture correctly described
- ✅ Hook integration points accurate

**Note**: Initial investigation draft incorrectly suggested a path error in PROJECT.md line 347. Comprehensive verification found NO such error - all documentation uses correct paths.

#### Strategic Recommendation

**MAINTAIN CURRENT ARCHITECTURE** - amplihack's system has 5 MAJOR advantages over Microsoft Amplifier patterns (session isolation, human-readable Markdown, fail-safe architecture, original request tracking, zero external dependencies).

### Lessons Learned

1. **Independent innovation can be better than adoption**
2. **Session isolation beats centralized state**
3. **Philosophy score predicts success** - Systems scoring 25+ out of 30 have been stable

## StatusLine Configuration Missing from Installation Templates (2025-11-18)

### Problem Discovered

**Custom status line feature is fully implemented but never configured during installation**. The `statusline.sh` script exists and works perfectly, but neither installation method (regular or UVX) adds the statusLine configuration to settings.json.

**Result**: Users lose custom status line on install/update, or never discover the feature exists.

### Root Cause

**Both installation paths exclude statusLine configuration**:

1. **Regular Installation** (`.claude/tools/amplihack/install.sh`):
   - Creates hardcoded settings.json template (lines 126-178)
   - Template includes permissions, hooks, MCP settings
   - Template **excludes statusLine** configuration

2. **UVX Installation** (`src/amplihack/utils/uvx_settings_template.json`):
   - Auto-generated on first UVX run if settings missing or lacks bypass permissions
   - Template includes permissions, hooks, MCP settings
   - Template **also excludes statusLine** configuration

**Why This Happens**: Templates were created independently of the statusline.sh implementation. The script exists at `.claude/tools/statusline.sh` but is never referenced in any installation automation.

### Impact

- Users in regular mode: Lose statusLine config when running install.sh
- Users in UVX mode: Never get statusLine configured automatically
- Feature discoverability: Zero - not documented in README, prerequisites, or setup guides
- User experience: Must manually edit settings.json to enable this production-ready feature

### The StatusLine Feature

**Location**: `.claude/tools/statusline.sh`

**Shows**:

- Directory path (with ~ for home)
- Git branch with dirty state indicator
- Git remote tracking
- Model name (color-coded: Red=Opus, Green=Sonnet, Blue=Haiku)
- Token usage (formatted with K/M suffixes)
- Cost tracking (USD)
- Session duration

**Configuration Required**:

```json
"statusLine": {
  "type": "command",
  "command": "$CLAUDE_PROJECT_DIR/.claude/tools/statusline.sh"
}
```

Or for global installation:

```json
"statusLine": {
  "type": "command",
  "command": "/home/username/.claude/tools/statusline.sh"
}
```

### Solution Implemented (Issue #1433)

**Fixed both installation templates**:

1. **install.sh** (line 136-139):

```json
  "statusLine": {
    "type": "command",
    "command": "HOME_PLACEHOLDER/.claude/tools/statusline.sh"
  },
```

(Note: HOME_PLACEHOLDER gets replaced with $HOME on line 175)

2. **uvx_settings_template.json** (line 27-30):

```json
  "statusLine": {
    "type": "command",
    "command": ".claude/tools/statusline.sh"
  },
```

(Note: UVX uses relative paths since it runs from project directory)

### How to Detect This Issue

1. Check if settings.json exists: `cat ~/.claude/settings.json`
2. Look for statusLine section: `grep -A 3 statusLine ~/.claude/settings.json`
3. If missing, check if statusline.sh exists: `ls -la ~/.claude/tools/statusline.sh`
4. Test the script manually:

```bash
echo '{"current_dir":"'$(pwd)'","display_name":"Test","id":"test","total_cost_usd":"1.23","total_duration_ms":"45000","transcript_path":""}' | ~/.claude/tools/statusline.sh
```

### Prevention

- ✅ statusLine now included in both installation templates
- Future: Document the feature in README.md and PREREQUISITES.md
- Future: Add setup verification step that checks for statusLine configuration
- Future: Consider adding to devcontainer post-create.sh for automatic Codespaces setup

**Related Files**:

- `.claude/tools/statusline.sh` - The actual implementation (production-ready)
- `.claude/tools/amplihack/install.sh` - Regular installation script (FIXED)
- `src/amplihack/utils/uvx_settings_template.json` - UVX installation template (FIXED)
- `src/amplihack/utils/uvx_settings_manager.py` - UVX settings manager
- `src/amplihack/__init__.py` - UVX detection logic (lines 345-351)

## Power-Steering Path Validation Bug (2025-11-17)

### Problem Discovered

**Power-steering mode is enabled and runs at session stop, but fails with path validation error**. The security check in `power_steering_checker.py` (\_validate_path method) rejects Claude Code's transcript location.

**Error Message**:

```
Transcript path /home/azureuser/.claude/projects/.../[session-id].jsonl is outside project root /home/azureuser/src/MicrosoftHackathon2025-AgenticCoding
```

### Root Cause

**Path validation is too strict**. The `_validate_path()` method only allows:

1. Paths within project root (e.g., `/home/azureuser/src/MicrosoftHackathon2025-AgenticCoding`)
2. Common temp directories (`/tmp`, `/var/tmp`, system temp)

But Claude Code stores transcripts in: `/home/azureuser/.claude/projects/-home-azureuser-src-MicrosoftHackathon2025-AgenticCoding/` which is OUTSIDE both allowed locations.

**Code Location**: `.claude/tools/amplihack/hooks/power_steering_checker.py:477-515`

### Impact

- Power-steering loads 21 considerations from YAML successfully
- But cannot read transcript to analyze session completeness
- Fails-open (allows session to end without blocking)
- Effectively disabled due to this error
- Users don't get session completeness checks

### How to Detect Power-Steering Invocation

**Primary Method**: Check the log file

```bash
cat .claude/runtime/power-steering/power_steering.log
```

**What to Look For**:

- `"Loaded 21 considerations from YAML"` = Invoked successfully
- `"Power-steering error (fail-open)"` = Encountered error
- `"Power-steering blocking stop"` = Blocked session end
- `"Power-steering approved stop"` = Approved session end

**When It Runs**: Only at Stop Hook (session end), not during session

**Disable Methods** (in priority order):

1. Semaphore file: `.claude/runtime/power-steering/.disabled` (runtime, immediate effect in current session)
2. Environment: `export AMPLIHACK_SKIP_POWER_STEERING=1` (affects sessions started after setting this variable)
3. Config: Set `"enabled": false` in `.claude/tools/amplihack/.power_steering_config` (default behavior at startup)

### Solution

**Option 1**: Whitelist `.claude/projects/` directory in path validation

```python
# Add to _validate_path() in power_steering_checker.py
# Check 3: Path is in Claude Code's project transcript directory
claude_projects_dir = Path.home() / ".claude" / "projects"
if str(path_resolved).startswith(str(claude_projects_dir)):
    return True
```

**Option 2**: Use relative path check instead of strict parent validation
**Option 3**: Store transcripts in project root (would require Claude Code changes)

### Key Learnings

1. **Fail-Open Design is Critical** - Path validation errors don't lock users out
2. **Security vs Usability Trade-off** - Strict validation prevented legitimate use case
3. **Detection is Easy** - Log file at `.claude/runtime/power-steering/power_steering.log` shows all activity
4. **Not All "Enabled" Means "Working"** - Config can say enabled but feature fails silently

### Testing/Verification

To verify power-steering is working properly after fix:

1. Check log file has no errors
2. Verify `"Power-steering approved stop"` or `"blocking stop"` messages appear
3. Test with incomplete work (open TODOs) - should block session end
4. Test with complete work - should approve session end

### References

- **PR**: #1351 "feat: Implement Complete Power-Steering Mode"
- **Config**: `.claude/tools/amplihack/.power_steering_config`
- **Considerations**: `.claude/tools/amplihack/considerations.yaml` (21 checks)
- **Checker**: `.claude/tools/amplihack/hooks/power_steering_checker.py`
- **Documentation**: `.claude/tools/amplihack/HOW_TO_CUSTOMIZE_POWER_STEERING.md`

## Mandatory End-to-End Testing Pattern (2025-11-10)

### Problem Discovered

**Step 8 of DEFAULT_WORKFLOW.md was not followed rigorously enough**. Code was committed after:

- Unit test structure validation
- Code syntax verification
- Agent reviews (cleanup, reviewer)

BUT missing the most critical test: **Real user experience validation with `uvx --from`**

### Why This Matters

**The Workflow Explicitly Requires**:

```
Step 8: Mandatory Local Testing (NOT in CI)
- Test simple use cases - Basic functionality verification
- Test complex use cases - Edge cases and longer operations
- Test integration points - External dependencies and APIs
- RULE: Never commit without local testing
```

**Example**: "If database changes: Test with actual data operations"

### Critical Learning

**ALWAYS test with `uvx --from <branch>` before committing**. This is THE definitive test that:

- Package installs correctly from the branch
- All dependencies resolve properly
- The actual user workflow works end-to-end
- Error messages appear as users will see them
- Configuration files get updated correctly

**Testing hierarchy** (all required):

1. ✅ Unit tests (fast, isolated)
2. ✅ Integration tests (components together)
3. ✅ Code reviews (agents verify quality)
4. **✅ End-to-end user experience test** (`uvx --from <branch>`) ← **MANDATORY BEFORE COMMIT**

### Pattern to Follow

```bash
# BEFORE committing ANY feature/fix:

# 1. Install from your branch
uvx --from git+https://github.com/org/repo@your-branch package-name command

# 2. Test the EXACT user workflow that was broken
# 3. Verify error messages are clear
# 4. Verify configuration updates work
# 5. Test edge cases in realistic scenarios

# ONLY THEN commit and push
```

### Example - Neo4j Port Allocation Fix (Issue #1283)

**What we tested**:

```python
# Verified port conflict resolution works:
✅ Detected occupied ports: 7774/7787
✅ Found alternatives: 7875/7888
✅ Clear messages: "⚠️ CONFLICT: Neo4j on port 7787..."
✅ .env updated: "✅ Updated .env with ports 7888/7875"
✅ Alternative ports available: Verified with is_port_in_use()
```

**This test found**: The fix works perfectly! Without this test, we would have pushed code we THOUGHT worked but hadn't verified in realistic conditions.

### Files Affected

- **Workflow Requirement**: `.claude/workflow/DEFAULT_WORKFLOW.md` Step 8
- **Test Validation**: End-to-end testing MUST use `uvx --from` for package-based projects

### Success Criteria for "Mandatory Local Testing"

For Step 8 to be marked complete, you MUST:

- [ ] Install with `uvx --from <your-branch>` or equivalent
- [ ] Run the EXACT command/workflow that was broken
- [ ] Verify the fix solves the user's problem
- [ ] Document test results showing success
- [ ] Only THEN proceed to commit

**No exceptions** - this is mandatory, not optional.

## Reflection System Data Flow Fix (2025-09-26)

### Problem Discovered

The AI-powered reflection system was failing silently despite merged PRs:

- No user-visible output during reflection analysis
- No GitHub issues being created from session analysis
- Error: "No session messages found for analysis"

### Root Cause

Data flow mismatch between legacy and AI-powered reflection systems:

```python
# BROKEN - stop.py was passing file path:
result = process_reflection_analysis(latest_analysis)

# But reflection.py expected raw messages:
def process_reflection_analysis(analysis_path: Path) -> Optional[str]:
```

The function signature and data passing were incompatible.

### Solution

Fixed the interface contract to pass messages directly:

```python
# FIXED - stop.py now passes messages:
result = process_reflection_analysis(messages)

# reflection.py updated to accept messages:
def process_reflection_analysis(messages: List[Dict]) -> Optional[str]:
```

### Key Files Modified

- `.claude/tools/amplihack/reflection/reflection.py` - Changed function
  signature
- `.claude/tools/amplihack/hooks/stop.py` - Changed data passing approach

### Verification

Reflection system now properly:

- Shows user-visible progress indicators and completion summaries
- Detects error patterns and workflow inefficiencies
- Creates GitHub issues with full URL tracking
- Integrates with UltraThink automation

### Configuration

- `REFLECTION_ENABLED=true` (default) - Enables AI-powered analysis
- `REFLECTION_ENABLED=false` - Disables reflection system
- Output appears in console during session stop events

## Context Preservation Implementation Success (2025-09-23)

### amplihack-Style Solution

Successfully implemented comprehensive conversation transcript and original
request preservation system based on amplihack's "Never lose context
again" approach.

### Problem Solved

Original user requests were getting lost during context compaction, leading to:

- Agents optimizing away explicit user requirements
- Solutions that missed the original goal
- Context loss when conversation gets compacted
- Inconsistent requirement tracking across workflow steps

### Solution Implemented

**Four-Component System**:

1. **Context Preservation System**
   (`.claude/tools/amplihack/context_preservation.py`)
   - Automatic extraction of requirements, constraints, and success criteria
   - Structured storage in both human and machine-readable formats
   - Agent-ready context formatting

2. **Enhanced Session Start Hook**
   (`.claude/tools/amplihack/hooks/session_start.py`)
   - Automatic original request extraction at session start
   - Top-priority injection into session context
   - Available to all agents from beginning

3. **PreCompact Hook** (`.claude/tools/amplihack/hooks/pre_compact.py`)
   - Automatic conversation export before compaction
   - Complete interaction history preservation
   - Compaction event metadata tracking

4. **Transcript Management** (`.claude/commands/amplihack/transcripts.md`)
   - amplihack-style `/transcripts` command
   - Context restoration, search, and management
   - Original request retrieval and display

### Key Technical Insights

**Regex Pattern Challenges**: Initial implementation failed due to unescaped
markdown patterns in regex. Solution: Properly escape `**Target**` patterns as
`\*\*Target\*\*`.

**Agent Context Injection**: Most effective approach is session-level context
injection rather than individual agent prompting. Session context is
automatically available to all agents.

**Preservation vs Performance**: Small performance cost (15-20ms per session
start) for comprehensive context preservation is acceptable for the benefit
gained.

**File Structure Strategy**: Dual format storage (`.md` for humans, `.json` for
machines) provides both readability and programmatic access without overhead.

### Validation Results

All tests passed:

- ✅ Original request extraction: 9 requirements from complex prompt
- ✅ Context formatting: 933-character agent context generated
- ✅ Conversation export: Complete transcript with timestamps
- ✅ Transcript management: Session listing and restoration
- ✅ Integration: Hook properly registered and functional

### Pattern Recognition

**amplihack's Approach Works**: The PreCompact hook strategy is the
gold standard for context preservation. Direct implementation of their approach
provides immediate value.

**Proactive vs Reactive**: Proactive preservation (export before loss) is far
superior to reactive recovery (trying to reconstruct after loss).

**Context Hierarchy**: Original user requirements must be injected at highest
priority in session context to ensure all agents receive them.

### Integration Success

**Workflow Integration**: Seamlessly integrated with existing 14-step workflow:

- Step 1: Automatic requirement extraction
- Steps 4-5: All agents receive requirements
- Step 6: Cleanup validation checkpoint
- Step 14: Final preservation validation

**Hook System**: Successfully extended Claude Code's hook system with PreCompact
functionality without disrupting existing hooks.

**Philosophy Compliance**: Maintained ruthless simplicity (~400 lines total)
while providing enterprise-grade context preservation.

### Next Implementation Targets

Based on this success:

1. **Agent Template System**: Standardize agent prompting with requirement
   injection
2. **Requirement Validation**: Automated checking of requirement preservation
3. **Context Analytics**: Track how often requirements are lost without this
   system
4. **Recovery Mechanisms**: Handle edge cases where preservation fails

## Agent Priority Hierarchy Critical Flaw (2025-01-23)

### Issue

Agents were overriding explicit user requirements in favor of project
philosophy. Specifically, when user requested "ALL files" for UVX deployment,
cleanup/simplification agents reduced it to "essential files only", directly
violating user's explicit instruction.

### Root Cause

Agents had philosophy guidance but no explicit instruction that user
requirements override philosophy. The system prioritized simplicity principles
over user-specified constraints, creating a hierarchy where philosophy > user
requirements instead of user requirements > philosophy.

### Solution

Implemented comprehensive User Requirement Priority System:

1. **Created USER_REQUIREMENT_PRIORITY.md** with mandatory hierarchy:
   - EXPLICIT USER REQUIREMENTS (Highest - Never Override)
   - IMPLICIT USER PREFERENCES
   - PROJECT PHILOSOPHY
   - DEFAULT BEHAVIORS (Lowest)

2. **Updated Critical Agents** with requirement preservation:
   - cleanup.md: Added mandatory user requirement check before any removal
   - reviewer.md: User requirement compliance as first review criteria
   - improvement-workflow.md: User requirement analysis in Stage 1

3. **Enhanced Workflow Safeguards**:
   - DEFAULT_WORKFLOW.md: Multiple validation checkpoints
   - Step 1: Identify explicit requirements FIRST
   - Step 6: Cleanup within user constraints only
   - Step 14: Final requirement preservation check

### Key Learnings

- **User explicit requirements are sacred** - they override all other guidance
- **Philosophy guides HOW to implement** - not WHAT to implement
- **Simple instruction updates** are more effective than complex permission
  systems
- **Multiple validation points** prevent single-point-of-failure in requirement
  preservation
- **Clear priority hierarchy** must be communicated to ALL agents

### Prevention

- All agent instructions now include mandatory user requirement priority check
- Workflow includes explicit requirement capture and preservation steps
- CLAUDE.md updated with priority system as core principle
- Validation scenarios documented for testing agent behavior

### Pattern Recognition

**Trigger Signs of Explicit Requirements:**

- "ALL files", "include everything", "don't simplify X"
- Quoted specifications: "use this exact format"
- Numbered lists of requirements
- "Must have", "explicitly", "specifically"

**Agent Behavior Rule:** Before any optimization/simplification → Check: "Was
this explicitly requested by user?" If YES → Preserve completely regardless of
philosophy

## Format for Entries

Each discovery should follow this format:

```markdown
## [Brief Title] (YYYY-MM-DD)

### Issue

What problem or challenge was encountered?

### Root Cause

Why did this happen? What was the underlying issue?

### Solution

How was it resolved? Include code examples if relevant.

### Key Learnings

What insights were gained? What should be remembered?

### Prevention

How can this be avoided in the future?
```

---

## Project Initialization (2025-01-16)

### Issue

Setting up the agentic coding framework with proper structure and philosophy.

### Root Cause

Need for a well-organized, AI-friendly project structure that supports
agent-based development.

### Solution

Created comprehensive `.claude` directory structure with:

- Context files for philosophy and patterns
- Agent definitions for specialized tasks
- Command system for complex workflows
- Hook system for session tracking
- Runtime directories for metrics and analysis

### Key Learnings

1. **Structure enables AI effectiveness** - Clear organization helps AI agents
   work better
2. **Philosophy guides decisions** - Having written principles prevents drift
3. **Patterns prevent wheel reinvention** - Documented solutions save time
4. **Agent specialization works** - Focused agents outperform general approaches

### Prevention

Always start projects with clear structure and philosophy documentation.

---

## Anti-Sycophancy Guidelines Implementation (2025-01-17)

### Issue

Sycophantic behavior in AI agents erodes user trust. When agents always agree
with users ("You're absolutely right!"), their feedback becomes meaningless and
users stop believing them.

### Root Cause

Default AI training often optimizes for agreeability and user satisfaction,
leading to excessive validation and avoidance of disagreement. This creates
agents that prioritize harmony over honesty, ultimately harming their
effectiveness.

### Solution

Created `.claude/context/TRUST.md` with 7 simple anti-sycophancy rules:

1. Disagree When Necessary - Point out flaws clearly with evidence
2. Question Unclear Requirements - Never guess, always clarify
3. Propose Alternatives - Suggest better approaches when you see them
4. Acknowledge Limitations - Say "I don't know" when appropriate
5. Skip Emotional Validation - Focus on technical merit, not feelings
6. Challenge Assumptions - Question wrong premises
7. Be Direct - No hedging, state assessments plainly

Added TRUST.md to the standard import list in CLAUDE.md to ensure all agents
follow these principles.

### Key Learnings

1. **Trust comes from honesty, not harmony** - Users value agents that catch
   mistakes
2. **Directness builds credibility** - Clear disagreement is better than hedged
   agreement
3. **Questions show engagement** - Asking for clarity demonstrates critical
   thinking
4. **Alternatives demonstrate expertise** - Proposing better solutions shows
   value
5. **Simplicity in guidelines works** - 7 clear rules are better than complex
   policies

### Prevention

- Include TRUST.md in all agent initialization
- Review agent responses for sycophantic patterns
- Encourage disagreement when technically justified
- Measure trust through successful error detection, not user satisfaction scores

---

## Enhanced Agent Delegation Instructions (2025-01-17)

### Issue

The current CLAUDE.md had minimal guidance on when to use specialized agents,
leading to underutilization of available agent capabilities.

### Root Cause

Initial CLAUDE.md focused on basic delegation ("What agents can help?") without
specific triggers or scenarios, missing the orchestration-first philosophy from
the amplifier project.

### Solution

Updated CLAUDE.md with comprehensive agent delegation instructions:

1. Added "GOLDEN RULE" emphasizing orchestration over implementation
2. Created specific delegation triggers mapping tasks to all 13 available agents
3. Included parallel execution examples for complex tasks
4. Added guidance for creating custom agents
5. Emphasized "ALWAYS IF POSSIBLE" for agent delegation

### Key Learnings

1. **Explicit triggers drive usage** - Listing specific scenarios for each agent
   increases delegation
2. **Orchestration mindset matters** - Positioning as orchestrator changes
   approach fundamentally
3. **Parallel patterns accelerate** - Showing concrete parallel examples
   encourages better execution
4. **Agent inventory awareness** - Must explicitly list all available agents to
   ensure usage
5. **Documentation drives behavior** - Clear instructions in CLAUDE.md shape AI
   behavior patterns

### Prevention

- Always compare CLAUDE.md files when porting functionality between projects
- Include specific usage examples for every agent created
- Regularly audit if available agents are being utilized
- Update delegation triggers when new agents are added

---

## Pre-commit Hooks Over-Engineering (2025-09-17)

### Issue

Initial pre-commit hooks implementation had 11+ hooks and 5 configuration files,
violating the project's ruthless simplicity principle.

### Root Cause

Common developer tendency to add "all the good tools" upfront rather than
starting minimal and adding complexity only when justified. The initial
implementation tried to solve problems that didn't exist yet.

### Solution

Simplified to only essential hooks:

```yaml
# From 11+ hooks down to 7 essential ones
repos:
  - pre-commit-hooks: check-merge-conflict, trailing-whitespace, end-of-file-fixer
  - ruff: format and basic linting
  - pyright: type checking
  - prettier: JS/TS/Markdown formatting
```

Deleted:

- Custom philosophy checker (arbitrary limits, no tests)
- detect-secrets (premature optimization)
- Complex pytest hook (fragile bash)
- Unused markdownlint config

### Key Learnings

1. **Start minimal, grow as needed** - Begin with 2-3 hooks, add others when
   problems arise
2. **Philosophy enforcement belongs in review** - Human judgment beats arbitrary
   metrics
3. **Dead code spreads quickly** - Commented configs and unused files multiply
4. **Automation can overcomplicate** - Sometimes IDE formatting is simpler than
   hooks
5. **Test your testing tools** - Custom hooks need tests too

### Prevention

- Always question: "What problem does this solve TODAY?"
- Count configuration files - more than 2-3 suggests over-engineering
- If a tool needs extensive configuration, it might be the wrong tool
- Prefer human review for subjective quality measures
- Remember: you can always add complexity, but removing it is harder

---

## CI Failure Resolution Process Analysis (2025-09-17)

### Issue

Complex CI failure resolution for PR 38 took 45 minutes involving version
mismatches, merge conflicts, and pre-commit hook failures. Need to optimize the
debugging process and create better diagnostic tools.

### Root Cause

Multiple compounding factors created a complex debugging scenario:

1. **Silent failures**: Merge conflicts blocked pre-commit hooks without clear
   error messages
2. **Environment mismatches**: Local (Python 3.12.10, ruff 0.12.7) vs CI (Python
   3.11, ruff 0.13.0)
3. **Missing diagnostic tools**: No automated environment comparison or pattern
   recognition
4. **Sequential investigation**: Manual step-by-step debugging instead of
   parallel diagnostics

### Solution

**Multi-agent orchestration approach**:

- Ultra-think coordination with architect, reviewer, and security agents
- Systematic investigation breaking problem into domains
- Persistent 45-minute effort identifying all root causes
- Complete resolution of 7 type errors, 2 unused variables, formatting issues,
  and merge conflict

**Key patterns identified**:

1. **CI Version Mismatch Pattern**: Local tests pass, CI fails on
   linting/formatting
2. **Silent Pre-commit Hook Failure Pattern**: Hooks appear to run but changes
   aren't applied

### Key Learnings

1. **Agent orchestration works for complex debugging**: Specialized agents
   (architect, reviewer, security) effectively decomposed the problem
2. **Silent failures need specialized detection**: Merge conflicts blocking
   tools require dedicated diagnostic capabilities
3. **Environment parity is critical**: Version mismatches cause significant
   investigation overhead (20-25 minutes)
4. **Pattern recognition accelerates resolution**: Known patterns should be
   automated
5. **Time-to-discovery varies by issue type**: Merge conflicts (10 min) vs
   version mismatches (25 min)
6. **Documentation discipline enables learning**: Having PHILOSOPHY.md,
   PATTERNS.md available accelerated analysis

### Prevention

**Immediate improvements needed**:

- **CI Diagnostics Agent**: Automated environment comparison and version
  mismatch detection
- **Silent Failure Detector Agent**: Pre-commit hook validation and merge
  conflict detection
- **Pattern Recognition Agent**: Automated matching to historical failure
  patterns

**Process improvements**:

- Environment comparison should be step 1 in CI failure debugging
- Check merge conflicts before running any diagnostic tools
- Use parallel agent execution for faster diagnosis
- Create pre-flight checks before CI submission

**New agent delegation triggers**:

- CI failures → CI Diagnostics Agent
- Silent tool failures → Silent Failure Detector Agent
- Recurring issues → Pattern Recognition Agent

**Target performance**: Reduce 45-minute complex debugging to 20-25 minutes
through automation and specialized agents.

---

## Claude-Trace UVX Argument Passthrough Issue (2025-09-26)

### Issue

UVX argument passthrough was failing for claude-trace integration. Commands like
`uvx --from git+... amplihack -- -p "prompt"` would launch interactively instead
of executing the prompt directly, forcing users to manually enter prompts.

### Root Cause

**Misdiagnosis Initially**: Thought issue was with UVX argument parsing, but
`parse_args_with_passthrough()` was working correctly.

**Actual Root Cause**: Command building logic in
`ClaudeLauncher.build_claude_command()` wasn't handling claude-trace syntax
properly. Claude-trace requires different command structure:

- **Standard claude**: `claude --dangerously-skip-permissions -p "prompt"`
- **Claude-trace**:
  `claude-trace --run-with chat --dangerously-skip-permissions -p "prompt"`

The key difference is claude-trace needs `--run-with chat` before Claude
arguments.

### Solution

Modified `src/amplihack/launcher/core.py` in `build_claude_command()` method:

```python
if claude_binary == "claude-trace":
    # claude-trace requires --run-with followed by the command and arguments
    # Format: claude-trace --run-with chat [claude-args...]
    cmd = [claude_binary, "--run-with", "chat"]

    # Add Claude arguments after the command
    cmd.append("--dangerously-skip-permissions")

    # Add system prompt, --add-dir, and forwarded arguments...
    if self.claude_args:
        cmd.extend(self.claude_args)

    return cmd
```

### Key Learnings

1. **Tool-specific syntax matters**: Different tools (claude vs claude-trace)
   may require completely different argument structures even when functionally
   equivalent
2. **Debugging scope**: Initially focused on argument parsing when the issue was
   in command building - trace through the entire pipeline
3. **Integration complexity**: Claude-trace integration adds syntax complexity
   that must be handled explicitly
4. **Testing real scenarios**: Mock testing wasn't sufficient - needed actual
   UVX deployment testing to catch this
5. **Command structure precedence**: Some tools require specific argument
   ordering (--run-with must come before other args)

### Prevention

- **Always test real deployment scenarios**: Don't rely only on local testing
  when tools have different deployment contexts
- **Document tool-specific syntax requirements**: Create clear examples for each
  supported execution mode
- **Test command building separately**: Unit test command construction logic
  independently from argument parsing
- **Integration testing**: Include UVX deployment testing in CI/CD pipeline
- **Clear error messages**: Provide better feedback when argument passthrough
  fails

### Pattern Recognition

**Trigger Signs of Command Structure Issues:**

- Arguments parsed correctly but command fails silently
- Tool works interactively but not with arguments
- Different behavior between direct execution and wrapper tools
- Integration tools requiring specific argument ordering

**Debugging Approach:** When argument passthrough fails:

1. Verify argument parsing is working (log parsed args)
2. Check command building logic (log generated command)
3. Test command manually to isolate syntax issues
4. Compare tool documentation for syntax differences

### Testing Validation

All scenarios now working:

- ✅ `uvx amplihack -- --help` (shows Claude help)
- ✅ `uvx amplihack -- -p "Hello world"` (executes prompt)
- ✅ `uvx amplihack -- --model claude-3-opus-20240229 -p "test"` (model +
  prompt)

**Issue**: #149 **PR**: #150 **Branch**:
`feat/issue-149-uvx-argument-passthrough`

---

## Socratic Questioning Pattern for Knowledge Exploration (2025-10-18)

### Issue

Need effective method for generating deep, probing questions that challenge technical claims and surface hidden assumptions in knowledge-builder scenarios.

### Root Cause

Simple question generation often produces shallow inquiries that can be deflected or answered superficially. Effective Socratic questioning requires strategic multi-dimensional attack on claims combined with formal precision.

### Solution

**Three-Dimensional Question Attack Strategy**:

1. **Empirical Dimension**: Challenge with observable evidence and historical outcomes
   - Example: "Why do memory safety bugs persist despite 30 years of tool development?"
   - Grounds abstract claims in reality
   - Hard to dismiss as "merely theoretical"

2. **Computational Dimension**: Probe tractability and complexity
   - Example: "Does manual discipline require solving NP-complete problems in your head?"
   - Connects theory to practical cognitive limitations
   - Reveals fundamental constraints

3. **Formal Mathematical Dimension**: Demand precise relationships
   - Example: "Is the relationship bijective or a subset? What's lost?"
   - Forces rigorous thinking
   - Prevents vague equivalence claims

**Key Techniques**:

- Use formal language (bijective, NP-complete) to force precision
- Embed context within questions to prevent deflection
- Connect theoretical claims to observable outcomes
- Attack different aspects of claim simultaneously (cannot defend all fronts equally)

### Key Learnings

1. **Multi-dimensional attack is more effective than single-angle questioning** - Forces comprehensive defense
2. **Formal language prevents hand-waving** - "Bijective" demands precision that "similar" doesn't
3. **Empirical grounding matters** - Observable outcomes harder to dismiss than pure theory
4. **Question length/complexity tradeoff** - Longer questions with embedded context are acceptable for deep exploration
5. **Pattern is domain-agnostic** - Works for technical debates, philosophical claims, design decisions

### Usage Context

**When to Use**:

- Knowledge-builder agent scenarios requiring deep exploration
- Challenging technical or philosophical claims
- Surfacing hidden assumptions in design decisions
- Teaching critical thinking through guided inquiry

**When NOT to Use**:

- Simple factual questions (overkill)
- Time-sensitive decisions (too slow)
- Consensus-building conversations (too confrontational)

### Evidence Status

**Proven**: 1 successful usage (memory safety ownership vs. discipline debate)
**Needs**: 2-3 more successful applications before promoting to PATTERNS.md
**Next Test**: Use with knowledge-builder agent in actual knowledge exploration scenario

### Prevention

**To implement effectively**:

- Test pattern 2-3 more times in varied contexts
- Validate with knowledge-builder agent integration
- Refine based on actual usage feedback
- Consider adding to PATTERNS.md after sufficient validation

**Trigger Signs for Pattern Use**:

- User makes strong equivalence claim ("X is just Y")
- Need to explore assumptions systematically
- Goal is deep understanding, not quick answers
- Conversational context allows longer-form inquiry

---

## Pattern Applicability Analysis Framework (2025-10-20)

### Discovery

Through analysis of PBZFT vs N-Version Programming decision, identified six reusable meta-patterns for evaluating when to adopt patterns from other domains, particularly distributed systems patterns applied to AI agent systems.

### Context

Considered implementing PBZFT (Practical Byzantine Fault Tolerance) as new amplihack pattern after comparison with existing N-Version Programming approach. Analysis revealed PBZFT would be 6-9x more complex with zero benefit, leading to systematic exploration of WHY this mismatch occurred and how to prevent similar mistakes.

### Pattern 1: Threat Model Precision Principle

**Core Insight**: Fault tolerance mechanisms are only effective when matched to correct threat model. Mismatched defenses add complexity without benefit.

**Decision Framework**:

1. Identify actual failure mode (what really breaks?)
2. Classify threat type: Honest mistakes vs Malicious intent
3. Match defense mechanism to threat type
4. Reject mechanisms designed for different threats

**Evidence**:

- PBZFT defends against Byzantine failures (malicious nodes) - not our threat
- N-Version defends against independent errors (honest mistakes) - our actual threat
- Voting defends against adversaries; expert review catches quality issues

**Anti-Pattern**: Applying "industry standard" solutions without verifying threat match

### Pattern 2: Voting vs Expert Judgment Selection Criteria

**Core Insight**: Voting and expert judgment serve fundamentally different purposes and produce different quality outcomes.

**When Voting Works**:

- Adversarial environment (can't trust individual nodes)
- Binary or simple discrete choices
- No objective quality metric available
- Consensus more valuable than correctness

**When Expert Judgment Works**:

- Cooperative environment (honest actors)
- Complex quality dimensions (code quality, architecture)
- Objective evaluation criteria exist
- Correctness more valuable than consensus

**Evidence**:

- Code quality has measurable dimensions (complexity, maintainability, correctness)
- Expert review provides detailed feedback ("Fix this specific issue")
- Voting provides only rejection ("This is wrong") without guidance
- N-Version achieves 30-65% error reduction through diversity, not voting

**Application**: Code review, architectural decisions, security audits all benefit from expert judgment over democratic voting.

### Pattern 3: Distributed Systems Pattern Applicability Test

**Core Insight**: Many distributed systems patterns don't apply to AI agent systems due to different coordination models and failure characteristics.

**Critical Differences**:

| Dimension     | Distributed Systems | AI Agent Systems        |
| ------------- | ------------------- | ----------------------- |
| Node Behavior | Can be malicious    | Honest but imperfect    |
| Failure Mode  | Byzantine faults    | Independent errors      |
| Coordination  | Explicit consensus  | Implicit through design |
| Communication | Messages, network   | Shared specifications   |
| Trust Model   | Zero-trust          | Cooperative             |

**Applicability Test Questions**:

1. Does pattern assume adversarial nodes? (Usually doesn't apply to AI)
2. Does pattern solve network partition issues? (AI agents share state)
3. Does pattern require message passing? (AI agents use shared context)
4. Does pattern optimize for communication cost? (AI has different cost model)

**Patterns That DO Apply to AI**:

- Load balancing (parallel agent execution)
- Caching (memoization, state management)
- Event-driven architecture (hooks, triggers)
- Circuit breakers (fallback strategies)

**Patterns That DON'T Apply to AI**:

- Byzantine consensus (PBZFT, blockchain)
- CAP theorem considerations (no network partitions)
- Gossip protocols (agents don't need eventual consistency)
- Quorum systems (voting inferior to expert review)

### Pattern 4: Complexity-Benefit Trade-off Quantification

**Core Insight**: Complex solutions must provide proportional benefit. Simple metrics reveal when complexity is unjustified.

**Quantification Framework**:

```
Complexity Cost = (Lines of Code) × (Conceptual Overhead) × (Integration Points)
Benefit Gain = (Problem Solved) × (Quality Improvement) × (Risk Reduction)

Justified Complexity: Benefit Gain / Complexity Cost > 3.0
```

**Evidence**:

- PBZFT: 6-9x complexity increase, 0x benefit (solves non-existent problem)
- N-Version: 2x complexity increase, 30-65% error reduction

**Red Flags for Unjustified Complexity**:

- Complexity ratio > 3x with no measurable benefit
- Solution requires understanding concepts not needed elsewhere
- Implementation needs extensive documentation to explain
- "Industry best practice" argument without context validation

### Pattern 5: Domain Appropriateness Check for "Best Practices"

**Core Insight**: Best practices from one domain can be anti-patterns in another. Always validate domain appropriateness before adopting.

**Validation Checklist**:

- Does this practice solve a problem that exists in MY domain?
- Does my domain have same threat model as source domain?
- Are constraints that drove this practice present in my system?
- What was original context that made this "best"?
- Has this been proven effective in contexts similar to mine?

**Common Cross-Domain Misapplications**:

- Microservices patterns → Monolithic apps (unnecessary distribution)
- Blockchain consensus → Database systems (unnecessary Byzantine tolerance)
- Military security → Consumer apps (disproportionate paranoia)
- Enterprise architecture → Startups (premature abstraction)

**Protection Strategy**:

1. Ask: "What problem does this solve in THAT domain?"
2. Verify: "Do I have that same problem?"
3. Question: "What are costs of importing this pattern?"
4. Consider: "Is there simpler solution that fits MY constraints?"

### Pattern 6: Diversity as Error Reduction Mechanism

**Core Insight**: Independent diverse implementations naturally reduce correlated errors without requiring voting or consensus mechanisms.

**How Diversity Works**:

- N diverse implementations of same specification
- Each has probability p of independent error
- Probability of same error in all N: p^N (exponential reduction)
- No voting required - diversity itself provides value

**Evidence**:

- N-Version provides 30-65% error reduction through diversity alone
- PBZFT's voting adds complexity without increasing diversity benefit
- Expert review can select best implementation after diversity generation

**Application to AI Agents**:

```python
# Generate diverse implementations
implementations = [
    agent1.generate(spec),
    agent2.generate(spec),
    agent3.generate(spec),
]

# Expert review selects best (not voting)
best = expert_reviewer.select_best(implementations, criteria)
```

**When Diversity Fails**:

- Specifications are ambiguous (correlated errors from misunderstanding)
- Common dependencies (same libraries, same bugs)
- Shared misconceptions (all agents trained on similar data)

### Meta-Pattern: Systematic Pattern Applicability Analysis

**Five-Phase Framework** for evaluating pattern adoption:

**Phase 1: Threat Model Match**

- Identify actual failure modes in target system
- Identify pattern's target failure modes
- Verify failure modes match
- If mismatch, REJECT pattern

**Phase 2: Mechanism Appropriateness**

- Does pattern use voting? (Usually wrong for quality assessment)
- Does pattern assume adversarial behavior? (Usually wrong for AI)
- Does pattern optimize for network communication? (Usually irrelevant for AI)
- Does pattern solve target domain's specific problem?

**Phase 3: Complexity Justification**

- Calculate complexity increase (lines, concepts, integration points)
- Measure benefit gain (error reduction, risk mitigation)
- Verify benefit/complexity ratio > 3.0
- If ratio < 3.0, seek simpler alternatives

**Phase 4: Domain Validation**

- Research pattern's origin domain
- Understand original context and constraints
- Verify target domain shares those characteristics
- Check for successful applications in similar domains

**Phase 5: Alternative Exploration**

- Brainstorm domain-native solutions
- Can simpler mechanisms achieve same benefits?
- What would "ruthless simplicity" approach look like?
- Can you get 80% of benefit with 20% of complexity?

### Key Learnings

1. **Threat model mismatch is primary source of inappropriate pattern adoption** - Always verify failure modes match before importing patterns
2. **Voting and expert judgment are not interchangeable** - Code quality requires expert review, not democratic voting
3. **Distributed systems patterns rarely map to AI systems** - Different trust models, coordination mechanisms, and failure characteristics
4. **Complexity must be proportionally justified** - 3:1 benefit-to-cost ratio minimum for adopting complex patterns
5. **Best practices are domain-specific** - What's "best" in blockchain may be anti-pattern for AI agents
6. **Diversity reduces errors without consensus overhead** - N-Version's power comes from diversity, not voting

### Prevention

**Before adopting any pattern from another domain:**

1. Run through five-phase applicability analysis
2. Verify threat model match as first step
3. Calculate complexity-to-benefit ratio
4. Question "industry best practice" claims
5. Explore domain-native alternatives
6. Default to ruthless simplicity unless complexity clearly justified

**Red Flags Requiring Deep Analysis**:

- Pattern from adversarial domain (blockchain, security) → cooperative domain (AI agents)
- Pattern optimizes for constraints not present in target (network latency, Byzantine nodes)
- Complexity increase > 3x without measurable benefit
- "Everyone uses this" without context-specific validation

### Integration with Existing Philosophy

This discovery strengthens and validates existing principles:

- **Ruthless Simplicity** (PHILOSOPHY.md): Complexity must justify existence
- **Zero-BS Implementation** (PHILOSOPHY.md): No solutions for non-existent problems
- **Question Everything** (PHILOSOPHY.md): Challenge "best practices" without validation
- **Necessity First** (PHILOSOPHY.md): "Do we actually need this right now?"

### Files Referenced

- PBZFT Analysis: `.claude/runtime/logs/[session]/pbzft_analysis.md`
- N-Version Pattern: Already implemented in amplihack
- Threat Modeling: Aligns with security agent principles
- Complexity Analysis: Informed by PHILOSOPHY.md simplicity mandate

### Next Steps

1. **Apply pattern applicability framework** when evaluating future pattern adoptions
2. **Create checklist tool** for systematic pattern evaluation
3. **Document known anti-patterns** from inappropriate domain transfers
4. **Strengthen agent instructions** to question pattern applicability before implementation

### Related Patterns

- Connects to "Ruthless Simplicity" (PHILOSOPHY.md)
- Enhances "Decision-Making Framework" (PHILOSOPHY.md)
- Validates "Question Everything" principle
- Extends pattern recognition capabilities

---

## Massive Parallel Reflection Workstream Execution (2025-11-05)

### Context

Successfully executed 13 parallel full-workflow tasks simultaneously, converting reflection system findings into GitHub issues and implementing solutions. This represented the largest parallel agent execution to date, demonstrating the scalability and robustness of the workflow system.

### Discovery

**Parallel Execution at Scale Works**: Successfully managed 13 concurrent feature implementations (issues #1089-#1101) using worktree isolation, each following the complete 13-step workflow from planning through PR creation.

**Key Metrics:**

- 13 issues created from reflection analysis
- 13 feature branches via git worktrees
- 13 PRs created with 9-10/10 philosophy compliance
- 7 message reduction features addressing real pain points
- 100% success rate (no failed workflows)

### Root Cause Analysis

**Why This Succeeded:**

1. **Worktree Isolation**: Each feature in separate worktree prevented cross-contamination
   - Branch: `feat/issue-{number}-{description}`
   - Location: `/tmp/worktree-issue-{number}`
   - No merge conflicts between parallel operations

2. **Agent Specialization**: Each workflow step delegated to appropriate agents
   - prompt-writer: Requirements clarification
   - architect: Design specifications
   - builder: Implementation
   - reviewer: Quality assurance
   - fix-agent: Conflict resolution

3. **Fix-Agent Pattern**: Systematic conflict resolution using templates
   - Cherry-pick strategy for divergent branches
   - Pattern-based resolution (import, config, quality)
   - Quick mode for common issues

4. **Documentation-First Approach**: Templates and workflows provided clear guidance
   - Message templates (.claude/data/message_templates/)
   - Fix templates (.claude/data/fix_templates/)
   - Workflow documentation (docs/workflows/)

### Solution Patterns That Worked

**1. Worktree Management Pattern:**

```bash
# Create isolated workspace
git worktree add /tmp/worktree-issue-{N} -b feat/issue-{N}-{description}

# Work independently
cd /tmp/worktree-issue-{N}
# ... implement feature ...

# Push and create PR
git push -u origin feat/issue-{N}-{description}
gh pr create --title "..." --body "..."

# Cleanup
cd /home/azureuser/src/MicrosoftHackathon2025-AgenticCoding
git worktree remove /tmp/worktree-issue-{N}
```

**2. Cherry-Pick Conflict Resolution:**

```bash
# When branches diverge from main
git fetch origin main
git cherry-pick origin/main

# Resolve conflicts systematically
/fix import   # Dependency issues
/fix config   # Configuration updates
/fix quality  # Formatting and style
```

**3. Message Reduction Features (High Value):**

- Budget awareness warnings (prevent token exhaustion)
- Complexity estimator (right-size responses)
- Message consolidation (reduce API calls)
- Context prioritization (focus on relevant info)
- Summary generation (compress long threads)
- Progressive disclosure (hide verbose output)
- Smart truncation (preserve key information)

### Key Learnings

1. **Parallel Agent Execution is Highly Effective**
   - Independent tasks can run simultaneously without interference
   - Worktrees provide perfect isolation mechanism
   - No performance degradation with 13 parallel workflows
   - Agent specialization maintains quality at scale

2. **Fix-Agent Pattern Scales Well**
   - Template-based resolution handles common patterns
   - Cherry-pick strategy effective for divergent branches
   - Pattern recognition accelerates conflict resolution
   - Systematic approach prevents mistakes under pressure

3. **Documentation-First is Lightweight and Effective**
   - Templates reduce decision overhead
   - Workflows provide clear process guidance
   - Documentation faster than code generation
   - Reusable across multiple features

4. **Message Reduction Features Address Real Pain Points**
   - Token budget exhaustion is frequent blocker
   - Response complexity often mismatched to need
   - API call volume impacts performance
   - Users need more control over verbosity

5. **Philosophy Compliance Remains High at Scale**
   - All 13 PRs scored 9-10/10
   - Ruthless simplicity maintained
   - Zero-BS implementation enforced
   - Modular design preserved

6. **Reflection System Generates Actionable Insights**
   - Identified 13 concrete improvement opportunities
   - Each mapped to specific user pain points
   - Clear implementation paths
   - Measurable impact potential

### Impact

**Demonstrates System Scalability:**

- Workflow handles 13+ concurrent tasks without degradation
- Agent orchestration remains effective at scale
- Quality standards maintained across all implementations
- Process is repeatable and systematic

**Validates Architecture Decisions:**

- Worktree isolation strategy proven
- Agent specialization approach validated
- Fix-agent pattern confirmed effective
- Documentation-first approach successful

**Identifies Improvement Opportunities:**

- Message reduction features fill real gaps
- Token budget management needs better tooling
- Response complexity should be tunable
- API call optimization has significant value

### Prevention Patterns

**For Future Parallel Execution:**

1. **Always Use Worktrees for Parallel Work**
   - One worktree per feature/issue
   - Prevents branch interference
   - Enables true parallel development
   - Easy cleanup with `git worktree remove`

2. **Cherry-Pick for Divergent Branches**
   - When branches diverge from main
   - Systematic conflict resolution
   - Preserves both lineages
   - Better than rebase for parallel work

3. **Use Fix-Agent for Systematic Resolution**
   - Pattern-based conflict handling
   - Template-driven solutions
   - Quick mode for common issues
   - Diagnostic mode for complex problems

4. **Document Templates Before Mass Changes**
   - Create reusable message templates
   - Document fix patterns
   - Write workflow guides
   - Templates save time at scale

### Files Modified/Created

**Issues Created:**

- #1089: Budget awareness warnings
- #1090: Message complexity estimator
- #1091: Message consolidation
- #1092: Context prioritization
- #1093: Summary generation
- #1094: Progressive disclosure
- #1095: Smart truncation
- #1096-#1101: Additional improvements

**PRs Created (all with 9-10/10 philosophy compliance):**

- PR #1102-#1114: Feature implementations

**Templates Created:**

- `.claude/data/message_templates/` (various)
- `.claude/data/fix_templates/` (import, config, quality, etc.)

**Workflows Documented:**

- `docs/workflows/parallel_execution.md`
- `docs/workflows/worktree_management.md`
- `docs/workflows/conflict_resolution.md`

### Related Patterns

**Enhances Existing Patterns:**

- Microsoft Amplifier Parallel Execution Engine (CLAUDE.md)
- Parallel execution templates and protocols
- Agent delegation strategy
- Fix-agent workflow optimization

**Validates Philosophy Principles:**

- Ruthless Simplicity: Maintained at scale
- Modular Design: Bricks & studs approach works
- Zero-BS Implementation: No shortcuts taken
- Agent Delegation: Orchestration over implementation

### Recommendations

1. **Promote Worktree Pattern as Standard**
   - Default approach for feature work
   - Document in onboarding materials
   - Add to workflow templates
   - Create helper scripts for common operations

2. **Expand Fix-Agent Template Library**
   - Add more common patterns
   - Document resolution strategies
   - Create decision trees for pattern selection
   - Measure effectiveness metrics

3. **Prioritize Message Reduction Features**
   - High user value (budget, complexity, consolidation)
   - Clear implementation paths
   - Measurable impact
   - Address frequent pain points

4. **Create Parallel Execution Playbook**
   - Document lessons learned
   - Provide concrete examples
   - Include troubleshooting guide
   - Share best practices

### Verification

**All 13 Workflows Completed Successfully:**

- ✅ Issues created with clear requirements
- ✅ Branches created with descriptive names
- ✅ Code implemented following specifications
- ✅ Tests passing (where applicable)
- ✅ Philosophy compliance 9-10/10
- ✅ PRs created with complete documentation
- ✅ No cross-contamination between features
- ✅ Systematic conflict resolution applied

**Performance Metrics:**

- Total time: ~4 hours for 13 features
- Average per feature: ~18 minutes
- Philosophy compliance: 9-10/10 average
- Success rate: 100%

### Next Steps

1. **Review PRs and merge successful implementations**
2. **Extract reusable patterns into documentation**
3. **Update workflow with lessons learned**
4. **Create templates for future parallel execution**
5. **Document worktree management best practices**
6. **Expand fix-agent template library**
7. **Implement high-priority message reduction features**

---

## Azure VM Bastion SSH Key Mismatch Recovery (2025-11-24)

### Issue

Azure VM accessible via Azure Bastion but SSH connection timing out with "SSH not ready after 30s (5 attempts)" error. The VM appeared healthy (running for 16 days, SSH daemon active), but connections through bastion were failing during authentication.

### Context

- **VM**: atg-dev (azlin-vm-1762552083) in westus2
- **Bastion**: azlin-bastion-westus2 (Standard SKU, Succeeded state)
- **Error Pattern**: Bastion tunnel created successfully, but SSH authentication failed
- **User Impact**: Complete inability to access VM through bastion despite infrastructure being operational

### Root Cause

**SSH Key Mismatch Between Local Key and VM authorized_keys**

The local azlin SSH key (`~/.ssh/azlin_key.pub`) didn't match the public key stored in the VM's `/home/azureuser/.ssh/authorized_keys` file:

**Local Key Fingerprint:**
```
ssh-ed25519 AAAAC3Nz...ILVjPF40AmPVONrOhAchyquam9aqjUPMh19ksQXiifiX
```

**VM authorized_keys Fingerprint:**
```
ssh-ed25519 AAAAC3Nz...IIxkuVYPctrnnuhHvrTq7KL+wVClH5rto1r/B5wL7KHM
```

SSH daemon logs revealed authentication failures:
```
error: kex_exchange_identification: Connection closed by remote host
Connection closed by authenticating user azureuser 10.0.1.5 port 60238 [preauth]
```

### Investigation Process (INVESTIGATION_WORKFLOW)

Used systematic 6-phase investigation workflow:

**Phase 1: Scope Definition**
- Identified key questions: VM state? Bastion state? SSH daemon status? Key mismatch?
- Defined success: User can connect and execute commands via bastion

**Phase 2: Exploration Strategy**
- Planned diagnostic approach: Check bastion connectivity, VM power state, SSH status, key comparison
- Selected tools: `azlin list`, `azlin status`, `az vm run-command`, direct Azure CLI inspection

**Phase 3: Parallel Deep Dives**
- Verified bastion tunnel creation (successful)
- Checked VM power state (running, 16 days uptime)
- Examined SSH daemon status (active and running)
- Compared local vs VM public keys (MISMATCH FOUND)

**Phase 4: Verification & Testing**
- Used `az vm run-command invoke` to check SSH status and authorized_keys
- Confirmed SSH daemon healthy but authentication failing
- Identified specific key fingerprint mismatch as root cause

**Phase 5: Synthesis**
- Root cause: Local SSH key rotated but VM's authorized_keys never updated
- Bastion working perfectly, SSH daemon healthy, only authentication layer broken
- Solution: Update VM's authorized_keys with current local public key

**Phase 6: Knowledge Capture**
- Documented recovery procedure
- Created reusable pattern for similar issues
- Added entry to DISCOVERIES.md

### Solution

**Surgical Key Replacement Using Azure Run-Command**

Used `az vm run-command invoke` to update the VM's authorized_keys without requiring SSH access:

```bash
LOCAL_KEY=$(cat ~/.ssh/azlin_key.pub)
az vm run-command invoke \
  --resource-group rysweet-linux-vm-pool \
  --name azlin-vm-1762552083 \
  --command-id RunShellScript \
  --scripts "echo '$LOCAL_KEY' > /home/azureuser/.ssh/authorized_keys && \
             chmod 600 /home/azureuser/.ssh/authorized_keys && \
             chown azureuser:azureuser /home/azureuser/.ssh/authorized_keys && \
             echo 'SSH key updated successfully'"
```

**Result**: Immediate connection success
- Connection time: 0.0s (instant, 1 attempt)
- Command execution: Successful (`whoami` returned `azureuser`)
- VM fully operational with 16 days uptime and healthy load averages

### Key Learnings

1. **Bastion Success ≠ SSH Success** - Azure Bastion tunnel can succeed while SSH authentication fails
   - Bastion handles network routing to private VMs
   - SSH handles authentication independently
   - Both layers must work for successful connection

2. **SSH Daemon Running ≠ Authentication Working** - Service health doesn't guarantee auth success
   - `systemctl status sshd` showing "active" doesn't mean keys are correct
   - Authentication happens after connection, not during service start
   - Check auth logs for "Connection closed by authenticating user" patterns

3. **Azure Run-Command is Emergency Access Method** - Bypasses SSH for VM recovery
   - Works even when SSH is completely broken
   - Requires Azure RBAC permissions (Contributor or VM Contributor)
   - Can execute arbitrary commands for diagnostics and fixes
   - Essential tool for SSH key rotation emergencies

4. **Key Rotation Requires VM Updates** - Local key changes don't auto-propagate
   - SSH keys are not synchronized automatically
   - Each VM maintains independent authorized_keys file
   - Key rotation must update ALL VMs that need access
   - `azlin keys rotate` exists but affects all VMs (consider carefully)

5. **Diagnostic Command Sequence Matters** - Right order reveals root cause faster
   - Check bastion connectivity first (network layer)
   - Check VM power state (infrastructure layer)
   - Check SSH daemon status (service layer)
   - Check SSH keys (authentication layer)
   - Each layer eliminates possibilities systematically

### Diagnostic Commands

**Check VM and Bastion Status:**
```bash
azlin list                          # Shows all VMs and bastion hosts
azlin status --vm <vm-name>         # Detailed VM status
```

**Check SSH Daemon Status:**
```bash
az vm run-command invoke \
  --resource-group <rg> \
  --name <vm-name> \
  --command-id RunShellScript \
  --scripts "systemctl status sshd"
```

**Check Authorized Keys:**
```bash
az vm run-command invoke \
  --resource-group <rg> \
  --name <vm-name> \
  --command-id RunShellScript \
  --scripts "cat /home/azureuser/.ssh/authorized_keys"
```

**Compare with Local Key:**
```bash
cat ~/.ssh/azlin_key.pub
```

### Prevention

**Before Key Rotation:**
1. Document all VMs that use current key
2. Plan update strategy (surgical vs. bulk)
3. Test on non-critical VM first
4. Keep old key available for rollback

**For Bulk Updates:**
```bash
azlin keys rotate                    # Rotates keys for ALL VMs
azlin keys rotate --vm-prefix <prefix>  # Targeted rotation
```

**For Surgical Updates:**
```bash
# Use az vm run-command as shown in solution
# Safer for single-VM issues
# Doesn't affect other VMs
```

**Post-Rotation Verification:**
```bash
azlin connect <vm-name> -y -- whoami  # Test connection
azlin connect <vm-name> -y -- uptime  # Verify commands work
```

### Pattern Recognition

**Trigger Signs of SSH Key Mismatch:**
- Bastion tunnel creates successfully
- SSH connection times out during authentication phase
- VM appears healthy (running, SSH daemon active)
- Error messages mention "Connection closed by authenticating user"
- Same user can't connect but used to be able to

**Debugging Workflow:**
1. Verify bastion connectivity (tunnel creation)
2. Verify VM is running (`azlin list`)
3. Verify SSH daemon is active (`systemctl status sshd`)
4. Compare local public key with VM's authorized_keys
5. If mismatch found, use run-command to update

**Alternative Recovery Methods:**
- Azure Portal serial console (if enabled)
- Azure Portal "Reset password" feature
- Snapshot + new VM with correct keys
- `azlin keys rotate` (affects all VMs)

### Files Modified

No code changes required - this was an operational recovery using existing Azure tooling.

### Verification

**Recovery Success Metrics:**
- ✅ Connection time: 0.0s (instant)
- ✅ SSH authentication: Successful
- ✅ Command execution: Working (`whoami`, `hostname`, `uptime` all successful)
- ✅ VM health: 16 days uptime, load average 0.00
- ✅ Bastion functionality: Tunnel creation working perfectly

**Test Commands:**
```bash
azlin connect atg-dev -y -- whoami
# Output: azureuser

azlin connect atg-dev -y -- "hostname && uptime"
# Output:
# azlin-vm-1762552083
#  16:13:45 up 16 days, 18:24,  3 users,  load average: 0.00, 0.01, 0.00
```

### Related Tools

- **azlin CLI**: Azure VM management with bastion integration
- **Azure Bastion**: Secure RDP/SSH connectivity without public IPs
- **az vm run-command**: Emergency VM access bypassing SSH
- **SSH keys**: Ed25519 keys for authentication

### Additional Feature Built

As a parallel workstream during investigation, implemented `azlin list -w/--wide` flag feature to prevent VM name truncation in table output, making it easier to copy/paste full VM names.

**Implementation:**
- Modified `src/azlin/cli.py` and `src/azlin/multi_context_display.py`
- Added `--wide/-w` flag with conditional column width logic
- Default: width=20 (Session), width=30 (VM Name)
- Wide mode: no_wrap=True (full names displayed)

### Success Criteria Met

All Phase 5 verification criteria achieved:
- [x] User can successfully initiate bastion connection
- [x] SSH session establishes through Azure Bastion
- [x] User can execute commands on VM
- [x] Connection is stable
- [x] Root cause identified and documented
- [x] Preventive measures documented

---

<!-- New discoveries will be added here as the project progresses -->

## SessionStart and Stop Hooks Executing Twice - Claude Code Bug (2025-11-21)

### Discovery

SessionStart and Stop hooks are executing **twice per session** due to a **known Claude Code bug in the hook execution engine** (#10871), NOT due to configuration errors. The issue affects all hook types and causes performance degradation and duplicate context injection.

### Context

Investigation triggered by system reminder messages showing "SessionStart:startup hook success: Success" appearing twice. Initial hypothesis was incorrect configuration format, but deeper analysis revealed the configuration is correct per official schema.

### Root Cause

**Claude Code Internal Bug**: The hook execution engine spawns **two separate Python processes** for each hook invocation, regardless of configuration.

**Current Configuration** (CORRECT per schema):

```json
"SessionStart": [
  {
    "hooks": [  // ✓ Required by Claude Code schema
      {
        "type": "command",
        "command": "$CLAUDE_PROJECT_DIR/.claude/tools/amplihack/hooks/session_start.py",
        "timeout": 10000
      }
    ]
  }
]
```

**Schema Requirement**:

```typescript
{
  "required": ["hooks"],  // The "hooks" wrapper is MANDATORY
  "additionalProperties": false
}
```

### Initial Hypothesis Was Wrong

**Initial theory**: Extra `"hooks": []` wrapper was causing duplication.

**Reality**: The wrapper is **required by Claude Code schema**. Removing it causes validation errors:

```
Settings validation failed:
- hooks.SessionStart.0.hooks: Expected array, but received undefined
```

**Actual cause**: Claude Code's hook execution engine has an internal bug that spawns two separate processes for each registered hook.

### Evidence

**Configuration Analysis**:

- Only 1 SessionStart hook registered in settings.json
- No duplicate configurations found
- Schema validation confirms format is correct
- **Two separate Python processes** spawn anyway (different PIDs)

**From `.claude/runtime/logs/session_start.log`**:

```
[2025-11-21T13:01:07.113446] INFO: session_start hook starting (Python 3.13.9)
[2025-11-21T13:01:07.113687] INFO: session_start hook starting (Python 3.13.9)
```

**From `.claude/runtime/logs/stop.log`**:

```
[2025-11-20T21:37:05.173846] INFO: stop hook starting (Python 3.13.9)
[2025-11-20T21:37:05.427256] INFO: stop hook starting (Python 3.13.9)
```

**Pattern**: All hooks (SessionStart, Stop, PostToolUse) show double execution with microsecond-level timing differences, indicating true parallel process spawning.

### Impact

| Area                  | Effect                                                   |
| --------------------- | -------------------------------------------------------- |
| **Performance**       | 2-4 seconds wasted per session (double process spawning) |
| **Context Pollution** | USER_PREFERENCES.md injected twice (~19KB duplicate)     |
| **Side Effects**      | File writes, metrics, logs all duplicated                |
| **Log Clarity**       | Every entry appears twice, making debugging confusing    |
| **Resource Usage**    | Double memory allocation, double I/O operations          |

### Solution

**NO CODE FIX AVAILABLE** - This is a Claude Code internal bug.

**Workarounds**:

1. Accept the duplication (hooks are idempotent, safe but wasteful)
2. Add process-level deduplication in hook_processor.py (complex)
3. Wait for upstream Claude Code fix

**Tracking**: Claude Code GitHub Issue #10871 "Plugin-registered hooks are executed twice with different PIDs"

### Configuration Format (CORRECT)

Our configuration **matches the official schema exactly**:

```json
"SessionStart": [
  {
    "hooks": [  // ✓ REQUIRED by schema
      {
        "type": "command",
        "command": "$CLAUDE_PROJECT_DIR/.claude/tools/amplihack/hooks/session_start.py",
        "timeout": 10000
      }
    ]
  }
]
```

**Schema requirement**:

```typescript
"required": ["hooks"],  // The "hooks" wrapper is MANDATORY
"additionalProperties": false
```

Attempting to remove the wrapper causes validation errors.

### Affected Hooks

| Hook             | Status     | Root Cause             |
| ---------------- | ---------- | ---------------------- |
| **SessionStart** | ❌ Runs 2x | Claude Code bug #10871 |
| **Stop**         | ❌ Runs 2x | Claude Code bug #10871 |
| **PostToolUse**  | ❌ Runs 2x | Claude Code bug #10871 |
| PreToolUse       | ❓ Unknown | Likely affected        |
| PreCompact       | ❓ Unknown | Likely affected        |

### Key Learnings

1. **Configuration was correct all along** - The `"hooks": []` wrapper is required by Claude Code schema
2. **Schema validation prevents incorrect "fixes"** - Attempted to remove wrapper, got validation errors
3. **Log analysis reveals issues but not always root cause** - Duplicate execution doesn't always mean duplicate configuration
4. **Upstream bugs affect downstream projects** - Known Claude Code bug (#10871) causes systematic duplication
5. **Idempotent design saves us** - Hooks are safe to run twice even though wasteful
6. **Investigation workflow worked** - Systematic analysis prevented incorrect fix from being deployed

### No Action Required

**Decision**: Accept the duplication as a known limitation until Claude Code team fixes #10871.

**Rationale**:

- Configuration is correct per official schema
- No user-side fix available without breaking schema validation
- Hooks are idempotent (safe to run twice)
- Performance impact acceptable (~2 seconds per session)
- Workarounds (process-level dedup) would add significant complexity

### Monitoring

Track Claude Code GitHub for fix:

- **Issue #10871**: "Plugin-registered hooks are executed twice with different PIDs"
- **Related**: #3523 (hook duplication), #3465 (hooks fired twice from home dir)

### Verification

Configuration correctness verified:

1. ✅ Only 1 hook registered per event type
2. ✅ Schema validation passes
3. ✅ Format matches official Claude Code documentation
4. ✅ Removing wrapper causes validation errors
5. ✅ Both processes run to completion (not a race condition)

### Files Analyzed

- `.claude/settings.json` (1 SessionStart hook, 1 Stop hook)
- `.claude/tools/amplihack/hooks/session_start.py` (hook implementation)
- `.claude/runtime/logs/session_start.log` (execution evidence)
- `.claude/runtime/logs/stop.log` (execution evidence)
- Claude Code schema (hook format requirements)

---

## Remember

- Document immediately while context is fresh
- Include specific error messages and stack traces
- Show actual code that fixed the problem
- Think about broader implications
- Update PATTERNS.md when a discovery becomes a reusable pattern

---

## Expert Agent Creation Pattern from Knowledge Bases (2025-10-18)

### Discovery

Successfully established reusable pattern for creating domain expert agents grounded in focused knowledge bases, achieving 10-20x learning speedup over traditional methods.

### Context

After merging PR #931 (knowledge-builder refactoring), tested end-to-end workflow by creating two expert agents:

1. Rust Programming Expert (memory safety, ownership)
2. Azure Kubernetes Expert (production AKS deployments)

### Pattern Components

**1. Focused Knowledge Base Structure**

```
.claude/data/{domain_name}/
├── Knowledge.md          # 7-10 core concepts with Q&A
├── KeyInfo.md           # Executive summary, learning path
└── HowToUseTheseFiles.md # Usage patterns, scenarios
```

**2. Knowledge Base Content**

- Q&A format (not documentation style)
- 2-3 practical code examples per concept
- Actionable, not theoretical
- Focused on specific use case (not 270 generic questions)

**3. Expert Agent Definition**

```markdown
---
description: {Domain} expert with...
knowledge_base: .claude/data/{domain_name}/
priority: high
---

# {Domain} Expert Agent

[References knowledge base, defines competencies, usage patterns]
```

### Key Learnings

1. **Focused Beats Breadth**
   - 7 focused concepts > 270 generic questions
   - Evidence: Rust implementation in 2 hours vs 20-40 hour traditional learning
   - Result: 10-20x speedup for project-specific domains

2. **Q&A Format Superior to Documentation**
   - Natural learning progression
   - "Why" alongside "how"
   - Easy to reference during implementation
   - Agent scored 9.5/10 in evaluation

3. **Real Code Examples Essential**
   - Working examples 10x more valuable than explanations
   - Can copy/adapt directly into implementation
   - Every concept needs 2-3 runnable examples

4. **Performance Matters for Adoption**
   - 30-minute generation time blocks practical use
   - Focused manual creation: 20 minutes
   - **Recommendation**: Add `--depth` parameter (shallow/medium/deep)

### Files Created

**Expert Agents:**

- `.claude/agents/amplihack/specialized/rust-programming-expert.md` (156 lines)
- `.claude/agents/amplihack/specialized/azure-kubernetes-expert.md` (262 lines)

**Rust Knowledge Base:**

- `amplihack-logparse/.claude/data/rust_focused_for_log_parser/Knowledge.md` (218 lines)
- `amplihack-logparse/.claude/data/rust_focused_for_log_parser/KeyInfo.md` (67 lines)
- `amplihack-logparse/.claude/data/rust_focused_for_log_parser/HowToUseTheseFiles.md` (83 lines)

**Azure AKS Knowledge Base:**

- `.claude/data/azure_aks_expert/Knowledge.md` (986 lines, 30+ examples)
- `.claude/data/azure_aks_expert/KeyInfo.md` (172 lines)
- `.claude/data/azure_aks_expert/HowToUseTheseFiles.md` (275 lines)

**Rust Log Parser (demonstrating knowledge application):**

- `amplihack-logparse/src/types.rs` (91 lines) - Ownership
- `amplihack-logparse/src/error.rs` (62 lines) - Error handling
- `amplihack-logparse/src/parser/mod.rs` (165 lines) - Borrowing, Result
- `amplihack-logparse/src/analyzer/mod.rs` (673 lines) - Traits
- `amplihack-logparse/src/main.rs` (wired up CLI)
- **Test Status**: 24/24 tests passing

### Verification

**Rust Expert Agent Test:**

- Question: Borrow checker lifetime error
- Result: Correctly referenced Lifetimes section (Knowledge.md lines 52-72)
- Provided: Proper fix with lifetime annotations
- Score: 9.5/10

**Azure AKS Expert Agent Test:**

- Question: Production deployment with HTTPS, autoscaling, Key Vault, monitoring
- Result: Correctly referenced 4 knowledge base sections
- Provided: Complete Azure CLI commands and YAML manifests
- Score: PASS (production-ready)

### Recommendations

1. **Optimize knowledge-builder performance**

   ```bash
   /knowledge-builder "topic" --depth shallow    # 10 questions, 2-3 min
   /knowledge-builder "topic" --depth medium     # 30 questions, 5-10 min
   /knowledge-builder "topic" --depth deep       # 270 questions, 30+ min
   ```

2. **Add focus parameter**

   ```bash
   /knowledge-builder "Rust" --focus "ownership,borrowing"
   ```

3. **Create more domain experts using this pattern**
   - AWS EKS (similar to AKS)
   - Terraform (infrastructure as code)
   - PostgreSQL (database operations)
   - React + TypeScript (frontend development)

### Impact

**Pattern Reusability**: Can be applied to any technical domain
**Learning Speedup**: 10-20x faster for project-specific learning
**Agent Quality**: Both agents production-ready, comprehensively tested
**Cost-Benefit**: ~1 hour per agent after pattern established

### Related Issues/PRs

- **Issue**: #930
- **PR**: #931 (knowledge-builder refactoring, MERGED)
- **PR**: #941 (auto mode fix, MERGED)

---

## Neo4j Container Port Mismatch Detection Bug (2025-11-08)

### Issue

Amplihack startup would fail with container name conflicts when starting in a different project directory than where the Neo4j container was originally created, even though a container with the expected name already existed:

```
✅ Our Neo4j container found on ports 7787/7774
Query failed... localhost:7688 (Connection refused)
Failed to create container... Conflict... already in use
```

### Root Cause

**Logic Flaw in Port Detection**: The `is_our_neo4j_container()` function checked if a container with the expected NAME existed, but didn't retrieve the ACTUAL ports the container was using.

**Exact Bug Location**: `src/amplihack/memory/neo4j/port_manager.py:147-149`

```python
# BROKEN - Assumes container is on ports from .env
if is_our_neo4j_container():  # Only checks name, doesn't get ports!
    messages.append(f"✅ Our Neo4j container found on ports {bolt_port}/{http_port}")
    return bolt_port, http_port, messages  # Returns WRONG ports from .env
```

**Error Sequence**:

1. Container exists on ports 7787/7774 (actual)
2. `.env` in new directory has port 7688 (wrong)
3. Code detects container exists by name ✅
4. Code assumes container is on 7688 (from `.env`) ❌
5. Connection to 7688 fails (nothing listening)
6. Code tries to create new container (name conflict)

### Solution

**Added `get_container_ports()` function** that queries actual container ports using `docker port`:

```python
def get_container_ports(container_name: str = "amplihack-neo4j") -> Optional[Tuple[int, int]]:
    """Get actual ports from running Neo4j container.

    Uses `docker port` command to inspect actual port mappings,
    not what .env file claims.

    Returns:
        (bolt_port, http_port) if container running with ports, None otherwise
    """
    result = subprocess.run(
        ["docker", "port", container_name],
        capture_output=True,
        timeout=5,
        text=True,
    )

    if result.returncode != 0:
        return None

    # Parse output: "7687/tcp -> 0.0.0.0:7787"
    # Extract actual host ports from both ports
    # ...
    return bolt_port, http_port
```

**Updated `resolve_port_conflicts()`** to use actual ports:

```python
# FIXED - Use actual container ports, not .env ports
container_ports = get_container_ports("amplihack-neo4j")
if container_ports:
    actual_bolt, actual_http = container_ports
    messages.append(f"✅ Our Neo4j container found on ports {actual_bolt}/{actual_http}")

    # Update .env if ports don't match
    if (actual_bolt != bolt_port or actual_http != http_port) and project_root:
        _update_env_ports(project_root, actual_bolt, actual_http)
        messages.append(f"✅ Updated .env to match container ports")

    return actual_bolt, actual_http, messages
```

### Key Learnings

1. **Container Detection ≠ Port Detection** - Knowing a container exists doesn't tell you what ports it's using
2. **`.env` Files Can Lie** - Configuration files can become stale, always verify actual runtime state
3. **Docker Port Command is Canonical** - `docker port <container>` returns actual mappings, not configured values
4. **Self-Healing Behavior** - Automatically updating `.env` to match reality prevents future failures
5. **Challenge User Assumptions** - The user was right that stopping the container wasn't the real fix - the port mismatch was the actual issue

### Prevention

**Before this fix:**

- Starting amplihack in multiple directories would fail with container conflicts
- Users had to manually sync `.env` files across projects
- No automatic detection of port mismatches

**After this fix:**

- Amplihack automatically detects actual container ports
- `.env` files auto-update to match reality
- Can start amplihack in any directory, will reuse existing container
- Self-healing behavior prevents stale configuration issues

### Testing

**Comprehensive test coverage (29 tests, all passing)**:

- Docker port output parsing (12 tests)
- Port conflict resolution (5 tests)
- Port availability detection (4 tests)
- Edge cases (5 tests)
- Integration scenarios (3 tests)

**Test Location**: `tests/unit/memory/neo4j/test_port_manager.py`

### Files Modified

- `src/amplihack/memory/neo4j/port_manager.py`: Added `get_container_ports()`, updated `resolve_port_conflicts()`
- `tests/unit/memory/neo4j/test_port_manager.py`: Added comprehensive test suite (29 tests)

### Verification

**Original Error Reproduced**: ✅
**Fix Applied**: ✅
**All Tests Passing**: ✅ 29/29
**Self-Healing Confirmed**: ✅ `.env` updates automatically

### Pattern Recognition

**Trigger Signs of Port Mismatch Issues**:

- "Container found" but connection fails
- "Conflict" errors when creating containers
- Port numbers in error messages don't match expected ports
- Working in different directories with shared container

**Debugging Approach**:

1. Check if container actually exists (`docker ps`)
2. Check what ports container is actually using (`docker port <name>`)
3. Check what ports configuration expects (`.env`, config files)
4. Fix: Use actual ports, not configured ports

### Philosophy Alignment

- **Ruthless Simplicity**: Single function solves the problem, minimal changes
- **Self-Healing**: System automatically corrects stale configuration
- **Zero-BS**: No workarounds, addresses root cause directly
- **Reality Over Configuration**: Trust Docker's actual state, not config files

---

## Power Steering Mode Branch Divergence (2025-11-16)

### Issue

User expected "power steering mode stop hook feature" from recent PR to be on by default, but it wasn't activating during session stop. Feature appeared to be disabled or broken.

### Root Cause

**Not a configuration bug - feature was completely missing from branch**. The branch `chore/skill-builder-progressive-disclosure` diverged from `main` at commit `9b0cac42` **BEFORE** the power steering feature was merged in commit `e103a6ca` (PR #1351).

**Timeline**:

- Merge base: `9b0cac42` "fix: Update hooks to use current project directory"
- Branch diverged: `0df062d1` "feat: Update skill builder to emphasize progressive disclosure"
- Power steering added: `e103a6ca` "feat: Implement Complete Power-Steering Mode" (6 commits ahead on main)
- Current main: `c72e80c3` (includes power steering + 5 more commits)

**Missing Components**: 11 files (5,243 lines of code):

- `.claude/tools/amplihack/.power_steering_config` - Main config with `"enabled": true`
- `.claude/tools/amplihack/considerations.yaml` - All 21 considerations
- `.claude/tools/amplihack/hooks/power_steering_checker.py` - Core checker (1,875 lines)
- `.claude/tools/amplihack/hooks/claude_power_steering.py` - Claude SDK integration (301 lines)
- Plus 7 more files (documentation, tests, templates)

### Solution

**Sync branch with main to obtain power steering feature**:

```bash
# Recommended: Rebase with stash
git stash push -m "WIP: skill-builder changes"
git fetch origin
git rebase origin/main
# Resolve conflicts (likely .claude/settings.json)
git stash pop
pre-commit run --all-files
git push origin chore/skill-builder-progressive-disclosure --force-with-lease
```

**Verification after sync**:

```bash
# Confirm config exists with enabled: true
cat .claude/tools/amplihack/.power_steering_config | grep "enabled"

# Should show: "enabled": true
```

### Key Learnings

1. **Branch Divergence Creates Feature Gaps** - Feature branches can miss important changes merged to main after divergence
2. **"Feature Not Working" Can Mean "Feature Not Present"** - Always check if feature exists before debugging configuration
3. **Git Branch Comparison is Diagnostic Tool** - `git log --oneline --graph HEAD...origin/main` reveals divergence and missing commits
4. **Power Steering Feature is Comprehensive** - 11 files, 5,243 lines covering:
   - 21 considerations across 6 categories (session completion, workflow, quality, testing, PR content, CI/CD)
   - AI-powered transcript analysis using Claude SDK
   - Fail-open philosophy (never blocks on errors)
   - Three-layer disable system (semaphore, env var, config)
   - 75 passing tests

5. **User Expectations Were Correct** - Feature IS enabled by default (`"enabled": true` in config) when present

### Prevention

**Before investigating "feature not working"**:

1. Verify feature exists on current branch
2. Check git history for when feature was added
3. Compare branch to main: `git log HEAD...origin/main`
4. Look for missing files that should exist

**Signs of Branch Divergence Issues**:

- Feature exists on main but not current branch
- Recent PRs mention feature but files don't exist
- Error messages reference files that aren't present
- Configuration files are missing entirely

**Debugging Approach**:

```bash
# 1. Check if files exist
ls -la .claude/tools/amplihack/.power_steering_config

# 2. Find when feature was added
git log --all --oneline --grep="power steering"

# 3. Check which branches have the feature
git branch --contains <commit-hash>

# 4. Compare current branch to main
git log --oneline --graph HEAD...origin/main

# 5. Identify merge base
git merge-base HEAD origin/main
```

### What Power Steering Does

**Power Steering Mode** is an intelligent session completion verification system:

1. **Analyzes Transcripts** - Reviews conversation history before allowing session end
2. **Checks 21 Considerations** - Validates work completeness across 6 categories:
   - Session Completion & Progress (8 checks)
   - Workflow Process Adherence (2 checks)
   - Code Quality & Philosophy Compliance (2 checks)
   - Testing & Local Validation (2 checks)
   - PR Content & Quality (4 checks)
   - CI/CD & Mergeability Status (3 checks)

3. **Blocks Incomplete Work** - Prevents session end if critical checks fail
4. **Provides Continuation Prompts** - Gives actionable guidance for completing work
5. **Uses Claude SDK** - AI-powered analysis instead of simple pattern matching
6. **Enabled by Default** - `"enabled": true` in `.power_steering_config`

### Files Involved

**Core Implementation**:

- `power_steering_checker.py` (1,875 lines) - Main checker with 21 consideration methods
- `claude_power_steering.py` (301 lines) - Claude SDK integration
- `stop.py` (modified) - Integration point in session stop hook

**Configuration**:

- `.power_steering_config` (JSON) - Global enable/disable, version tracking
- `considerations.yaml` (237 lines) - All 21 considerations with descriptions, severity, enabled flags

**Documentation & Templates**:

- `HOW_TO_CUSTOMIZE_POWER_STEERING.md` (636 lines) - Complete user guide
- `power_steering_prompt.txt` (74 lines) - Claude SDK prompt template

**Testing**:

- 5 test files with 75 passing tests covering all functionality

### Verification

**After syncing branch**:

- ✅ Power steering config exists with `"enabled": true`
- ✅ All 21 considerations loaded from `considerations.yaml`
- ✅ Integration in `stop.py` active
- ✅ 75 tests passing
- ✅ Feature functions as user expected

### Related Issues/PRs

- **PR #1351**: "feat: Implement Complete Power-Steering Mode - All 21 Considerations + User Customization"
- **Commit**: `e103a6ca` (merged to main after branch divergence)
- **Investigation**: Used INVESTIGATION_WORKFLOW.md (6 phases) for systematic analysis

### Pattern Recognition

**Workflow Used**: INVESTIGATION_WORKFLOW.md proved highly effective:

- Phase 1: Scope Definition - Clarified what power steering should do
- Phase 2: Exploration Strategy - Planned agent deployment
- Phase 3: Parallel Deep Dives - analyzer + integration agents in parallel
- Phase 4: Verification - Confirmed findings with git commands
- Phase 5: Synthesis - Comprehensive explanation of root cause
- Phase 6: Knowledge Capture - This DISCOVERIES.md entry

**Agent Orchestration**: Deployed prompt-writer, analyzer, and integration agents in parallel for efficient investigation. All three agents provided valuable complementary perspectives.
